<!doctype html>
<html lang="en" data-bs-theme="light">
  <head>
    <meta charset="utf-8"></meta>
    <meta name="viewport" content="width=device-width, initial-scale=1"></meta>
    <title>logicalmethods.ai &ndash; Probability and inductive logic </title>
    <link rel="icon" type="image/x-icon" href="https://logicalmethods.ai/favicon.ico"></link>
    <link href="https://logicalmethods.ai/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet"></link>
    <link rel="stylesheet" href="https://logicalmethods.ai/icons/font/bootstrap-icons.min.css"></link>
    <link rel="stylesheet" href="https://logicalmethods.ai/css/layout.css"></link>
    <link rel="stylesheet" href="https://logicalmethods.ai/css/syntax_hl.css"></link>
    <link href="https://logicalmethods.ai/css/textbook.css" rel="stylesheet"></link>
    <style>
      
    :root {
      --chapter: "11";
    }

    </style>
    
      <link rel="stylesheet" href="https://logicalmethods.ai/katex/katex.min.css">
<script defer src="https://logicalmethods.ai/katex/katex.min.js"></script>
<script defer src="https://logicalmethods.ai/katex/contrib/auto-render.min.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},     
        {left: '$', right: '$', display: false},     
        {left: '\\[', right: '\\]', display: true},   
        {left: '\\(', right: '\\)', display: false},  
      ],
      throwOnError : false
    });
  });
</script>


    
  </head>
  <body class="bg-black d-flex flex-column m-0 p-0">
    <div class="offcanvas offcanvas-start" tabindex="-1" id="offcanvas-nav" aria-labelledby="offcanvas-navLabel" data-bs-theme="dark" data-bs-backdrop="false">
  <div class="offcanvas-header">
    <div class="h5 offcanvas-title" id="offcanvas-navLabel">
      Browse course
    </div>
    <button type="button" class="btn-close" data-bs-dismiss="offcanvas" aria-label="Close"></button>
  </div>
  
  <div class="offcanvas-body">
    <ul class="navbar-nav">
      
      <li class="nav-item">
        <div class="d-flex flex-row">
          <div class="container">
            <a class="nav-link " href="/about/"><i class="bi-file-person-fill"></i> &nbsp;&nbsp; About</a>
          </div>
      </li>
      
      
      <hr>
      <li class="nav-item">
        <div class="d-flex flex-row">
          <div class="container">
            <a class="nav-link fw-bold active" href="/textbook/"><i class="bi bi-book"></i> &nbsp;&nbsp; Textbook</a>
          </div>
          <button 
            class="btn btn-sm btn-dark" 
            href="/textbook/"
            id="navbarSectiontxt-home" data-bs-toggle="collapse"
            data-bs-target="#collapsetxt-home"  
            aria-expanded="true"
            aria-controls="collapsetxt-home">
            <i class="bi bi-caret-left-fill toggle-icon"></i>
          </button>
        </div>
        <div class="collapse show" id="collapsetxt-home">
          <ul class="list-unstyled ms-3">
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/logic-and-ai/"
              >1. 
                <i class="bi bi-unlock-fill"></i>
                 Logic and AI
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/formal-languages/"
              >2. 
                <i class="bi bi-unlock-fill"></i>
                 Formal languages
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/valid-inference/"
              >3. 
                <i class="bi bi-unlock-fill"></i>
                 Valid Inference
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/boolean/"
              >4. 
                <i class="bi bi-unlock-fill"></i>
                 Boolean algebra
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/sat/"
              >5. 
                <i class="bi bi-unlock-fill"></i>
                 Boolean satisfiability
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/conditionals/"
              >6. 
                <i class="bi bi-unlock-fill"></i>
                 Logical conditionals
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/proofs/"
              >7. 
                <i class="bi bi-unlock-fill"></i>
                 Logical proofs
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/fol/"
              >8. 
                <i class="bi bi-unlock-fill"></i>
                 FOL
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/fol-inference/"
              >9. 
                <i class="bi bi-unlock-fill"></i>
                 FOL Inference
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/many-valued/"
              >10. 
                <i class="bi bi-unlock-fill"></i>
                 Many-valued logics
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                fw-bold active
                
                "
                href="/textbook/probability/"
              >11. 
                <i class="bi bi-unlock-fill"></i>
                 Probability and inductive logic
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                link-opacity-50
                "
                href="javascript:;"
              >12. 
                <i class="bi bi-lock-fill"></i>
                 Logic-based learning
              </a>
            </li>
            
          </ul>
        </div>
          
      </li>
      
      
      <hr>
      <li class="nav-item">
        <div class="d-flex flex-row">
          <div class="container">
            <a class="nav-link " href="/slides/"><i class="bi bi-rocket-takeoff-fill"></i> &nbsp;&nbsp; Slides</a>
          </div>
          <button 
            class="btn btn-sm btn-dark" 
            href="/slides/"
            id="navbarSectionsli-home" data-bs-toggle="collapse"
            data-bs-target="#collapsesli-home"  
            aria-expanded="aria-expanded=false"
            aria-controls="collapsesli-home">
            <i class="bi bi-caret-left-fill toggle-icon"></i>
          </button>
        </div>
        <div class="collapse " id="collapsesli-home">
          <ul class="list-unstyled ms-3">
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/teaser/"
              >0. 
                <i class="bi bi-unlock-fill"></i>
                 Preamble
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/logic-and-ai/"
              >1. 
                <i class="bi bi-unlock-fill"></i>
                 Logic and AI
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/formal-languages/"
              >2. 
                <i class="bi bi-unlock-fill"></i>
                 Formal languages
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/valid-inference/"
              >3. 
                <i class="bi bi-unlock-fill"></i>
                 Valid inference
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/boolean/"
              >4. 
                <i class="bi bi-unlock-fill"></i>
                 Boolean algebra
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/sat/"
              >5. 
                <i class="bi bi-unlock-fill"></i>
                 Boolean satisfiability
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/conditionals/"
              >6. 
                <i class="bi bi-unlock-fill"></i>
                 Logical conditionals
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/proof/"
              >7. 
                <i class="bi bi-unlock-fill"></i>
                 Logical proofs
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/fol/"
              >8. 
                <i class="bi bi-unlock-fill"></i>
                 FOL
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/fol-inference/"
              >9. 
                <i class="bi bi-unlock-fill"></i>
                 FOL Inference
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/many-valued/"
              >10. 
                <i class="bi bi-unlock-fill"></i>
                 Many-valued Logic
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/probability/"
              >11. 
                <i class="bi bi-unlock-fill"></i>
                 Probability and inductive logic
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                link-opacity-50
                "
                href="javascript:;"
              >12. 
                <i class="bi bi-lock-fill"></i>
                 Logical learning
              </a>
            </li>
            
          </ul>
        </div>
          
      </li>
      
      
      <hr>
      <li class="nav-item">
        <div class="d-flex flex-row">
          <div class="container">
            <a class="nav-link " href="/exercises/"><i class="bi-gear-fill"></i> &nbsp;&nbsp; Exercises</a>
          </div>
          <button 
            class="btn btn-sm btn-dark" 
            href="/exercises/"
            id="navbarSectionexe-home" data-bs-toggle="collapse"
            data-bs-target="#collapseexe-home"  
            aria-expanded="aria-expanded=false"
            aria-controls="collapseexe-home">
            <i class="bi bi-caret-left-fill toggle-icon"></i>
          </button>
        </div>
        <div class="collapse " id="collapseexe-home">
          <ul class="list-unstyled ms-3">
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/preamble/"
              >0. 
                <i class="bi bi-unlock-fill"></i>
                 Preamble
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/logic-and-ai/"
              >1. 
                <i class="bi bi-unlock-fill"></i>
                 Logic and AI
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/formal-languages/"
              >2. 
                <i class="bi bi-unlock-fill"></i>
                 Formal languages
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/valid-inference/"
              >3. 
                <i class="bi bi-unlock-fill"></i>
                 Valid inference
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/boolean/"
              >4. 
                <i class="bi bi-unlock-fill"></i>
                 Boolean algebra
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/sat/"
              >5. 
                <i class="bi bi-unlock-fill"></i>
                 Boolean satisfiability
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/conditionals/"
              >6. 
                <i class="bi bi-unlock-fill"></i>
                 Logical conditionals
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/proof/"
              >7. 
                <i class="bi bi-unlock-fill"></i>
                 Logical proofs
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/fol/"
              >8. 
                <i class="bi bi-unlock-fill"></i>
                 FOL
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/fol-inference/"
              >9. 
                <i class="bi bi-unlock-fill"></i>
                 FOL inference
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/many-valued/"
              >10. 
                <i class="bi bi-unlock-fill"></i>
                 Many-valued logics
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/probability/"
              >11. 
                <i class="bi bi-unlock-fill"></i>
                 Logic and probability
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                link-opacity-50
                "
                href="javascript:;"
              >12. 
                <i class="bi bi-lock-fill"></i>
                 Logical learning
              </a>
            </li>
            
          </ul>
        </div>
          
      </li>
      
      
      <hr>
      <li class="nav-item">
        <div class="d-flex flex-row">
          <div class="container">
            <a class="nav-link " href="/assignments/"><i class="bi bi-house-gear-fill"></i> &nbsp;&nbsp; Assignments</a>
          </div>
          <button 
            class="btn btn-sm btn-dark" 
            href="/assignments/"
            id="navbarSectionass-home" data-bs-toggle="collapse"
            data-bs-target="#collapseass-home"  
            aria-expanded="aria-expanded=false"
            aria-controls="collapseass-home">
            <i class="bi bi-caret-left-fill toggle-icon"></i>
          </button>
        </div>
        <div class="collapse " id="collapseass-home">
          <ul class="list-unstyled ms-3">
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/assignments/assignment_1/"
              >1. 
                <i class="bi bi-unlock-fill"></i>
                 Assignment 1 (due 09/19/2025)
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/assignments/assignment_2/"
              >2. 
                <i class="bi bi-unlock-fill"></i>
                 Assignment 2 (due 03/10/2025)
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/assignments/assignment_3/"
              >3. 
                <i class="bi bi-unlock-fill"></i>
                 Assignment 3 (due 24/10/2025)
              </a>
            </li>
            
          </ul>
        </div>
          
      </li>
      
      
      
      
      
    </ul>
  </div>
</div>



    <header class="align-self-center d-flex flex-column m-0 mb-md-1 p-0" data-bs-theme="dark">
      <nav class="navbar bg-dark shadow" data-bs-theme="dark" aria-label="header-navbar">
  <div class="container-sm align-self-center px-3">
    <a class="navbar-brand fs-6" href="https://logicalmethods.ai/">
    <img src="https://logicalmethods.ai/img/nav_id.png" class="inert-img img-fluid m-2" draggable="false" width="400px">
    </a>
    <button class="navbar-toggler border-0" type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas-nav" aria-controls="offcanvas-nav">
      <span class="navbar-toggler-icon"></span>
    </button>
  </div>
</nav>

      
      
    </header>
    
    <main class="align-self-center d-flex flex-column flex-fill bg-white m-md-1 p-0" data-bs-theme="light">
      <ul class="navbar-nav flex-row justify-content-center mt-2">

  <li class="nav-item">
    <a href="/textbook/" class="btn" style="font-size: 20pt;" tabindex="-1" role="button" aria-disabled="false">
      <i class="bi bi-house-up"></i>
    </a>
  </li>
</ul>
<div class="container chapter">
  <p>By: <em>Johannes Korbmacher</em></p>
  <div class="m-4"><h1 id="probability-and-inductive-logic">Probability and inductive logic<button class="btn btn-back-to-top">
    
  </button>
</h1>
<hr>

<p>
<img src="https://logicalmethods.ai/textbook/probability/img/llm_example.png" class="rounded  float-start inert-img img-fluid m-2"  width="350px"  alt="" >

Inductive inference plays a crucial role in AI technologies both on the low
level and on the high level. On the low level, inductive inference is, for
example, the logical foundation for <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Large_language_model"
  target="_blank">large language models
(LLMs)</a>, which in turn give
us <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Chatbot"
  target="_blank">chatbots</a> and other modern AI
technologies. In essence, an LLM is (an approximation of) a <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Probability_distribution"
  target="_blank">probability
distribution</a> over
<i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Large_language_model#Tokenization"
  target="_blank">tokens</a>—small-ish
sequences of characters that make up words. The idea is that we can use the
<i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Conditional_probability"
  target="_blank">conditional probability</a>
of one token given a sequence of others to predict what the <em>next</em>
token in the sequence should be. The perhaps surprising fact is, that this
&ldquo;next-token prediction&rdquo; allows us to develop AI agents with human-like
abilities.</p>
<p>
<img src="https://logicalmethods.ai/textbook/probability/img/llm_inference.png" class="rounded  float-end inert-img img-fluid m-2"  width="350px"  alt="" >

LLMs, and the technologies that they are based on, are the state of the art of
statistics-based AI: they are not based on hand-written if-then rules and
automated deductive inference, but on statistical models of large-scale
datasets, which are developed using advanced <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Machine_learning"
  target="_blank">machine learning
methods</a>, such as <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Gradient_descent"
  target="_blank">gradient
descent</a>. But the fact that LLMs
aren&rsquo;t
<i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence"
  target="_blank">logic-based</a>
doesn&rsquo;t mean that they have nothing to do with logic. In fact, next-token
prediction is a form of <em>(inductive) inference</em>: <code class="customCode">t₁, &hellip;, tₙ</code> are the previous
tokens, therefore <code class="customCode">s</code> is the next token. If we want to have any hope at all
of understanding how LLMs work on the low level, we need to study inductive
logic.</p>
<p>
<img src="https://logicalmethods.ai/textbook/probability/img/ai_spam.png" class="rounded  float-end inert-img img-fluid m-2"  width="150px"  alt="" >

In fact, inference that involves prediction is typically inductive—we can hardly
ever make predictions with certainty. In this chapter, we&rsquo;ll focus on predictive
inference on the higher, propositional level. As an example of an AI-technology
that features inductive inference we&rsquo;ll look at <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Email_filtering"
  target="_blank"><code class="customCode">SPAM</code>
filters</a>.</p>
<p>There are different approaches to inductive logic in AI-research. In the
logic-based tradition, there are systems which aim to assimilate inductive logic
to deductive logic:  they work just like the sytems we know from deductive logic
in terms of syntax, semantics, and proof-theory, except that their consequence
relation is inductive: it allows for the premises to be true and the conclusion
to be false. The result are systems of <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Non-monotonic_logic"
  target="_blank">non-monotonic
logic</a>, such as
<i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Autoepistemic_logic"
  target="_blank">autoepistemic logic</a> and
<i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Default_logic"
  target="_blank">default logic</a>, which have a solid
place in logic-based AI research.</p>
<p>But in broader terms, <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Probability_theory"
  target="_blank">probability
theory</a> is the most widely
used framework for inductive logic. In this chapter, we&rsquo;ll look at probability
through the lens of logical theory. As you&rsquo;ll see, we can develop the standard
theory of <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Probability_distribution#Discrete_probability_distribution"
  target="_blank">finite, discrete probability
distributions</a>
using the truth-tables we&rsquo;re familiar with from Boolean logic. This allows us to
develop the standard theory of inductive inference in the same setting as the
standard theory of deductive inference—which in turn makes it possible to
compare the two.</p>
<p>At the end of the chapter, you&rsquo;ll be able to:</p>
<ul>
<li>explain the basic concept of a probability distribution over a propositional
language,</li>
<li>define probability distributions using classical truth-tables and probability
weights,</li>
<li>calculate conditional probabilities using Kolmogorov&rsquo;s definition and <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Bayes%27_theorem"
  target="_blank">Bayes
formula</a>,</li>
<li>explain the relevance of probability to inductive inference and apply this in
examples, such as naive Bayes filters,</li>
<li>explain the logical relation between inductive and deductive inference.</li>
</ul>
<h2 id="probabilities">Probabilities<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h2>
<hr>

<p>Lets think back to our example of a six-sided die. When we&rsquo;re rolling the die,
we don&rsquo;t know what the outcome will be. A process that has different possible
outcomes is called <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Randomness"
  target="_blank">random</a> in
probability theory. In our case, there are six possible outcomes: we could roll
a 1, a 2, a 3, a 4, a 5, or a 6. Visually, we can represent these possible
outcomes as:

<img src="https://logicalmethods.ai/textbook/probability/img/die_space.png" class="rounded mx-auto d-block inert-img img-fluid"  width="500px"  alt="" >

In probability theory, the symbol <span class="excalifont">Ω</span> stands for the so-called <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Sample_space"
  target="_blank">sample
space</a>, from which our possible
outcomes are recruited. Rolling the die is what&rsquo;s called a <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Experiment_%28probability_theory%29"
  target="_blank">random
experiment</a>,
which is a procedure that decides the random process by settling the outcome.</p>
<p>Each outcome in our experiment has a chance of occurring, but what this chance
is depends on the die. On a <em>fair</em> die, each outcome is equally likely. On a
<i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Dice#Loaded_dice"
  target="_blank"><em>loaded</em> die</a>, instead, some
outcomes are more likely than others. So, there is no &ldquo;intrinsic&rdquo; chance that we
must assign to the outcomes given our setup of rolling a 6-sided die. There are
different possible ways of assigning chances, which correspond to different ways
the world could be like.</p>
<p>Formally, we model these different probabilities using what&rsquo;s called a
<i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Probability_mass_function"
  target="_blank">probability mass
function</a>, which is
often denoted <code class="customCode">p</code>. This is a mathematical function that assigns a value between
<code class="customCode">0</code> and <code class="customCode">1</code> (inclusive) to each of our outcomes to measure its chance of
occurring. The value <code class="customCode">0</code> means that it&rsquo;s impossible for the outcome to occur,
and the value <code class="customCode">1</code> means that it&rsquo;s absolutely certain. Intermediate values
measure the chance with a higher value meaning a higher chance.</p>
<p>The only constraint on a probability mass function is that the sum of the values
over all outcomes must be one, i.e.</p>
<div class="customBlock text-center my-4"><code class="customCode">
p(
<img src="/img/die_1.png" alt="die_1" class="inline-icon" style="height:1em;width:auto;"/>
) + p(
<img src="/img/die_2.png" alt="die_2" class="inline-icon" style="height:1em;width:auto;"/>
) + p(
<img src="/img/die_3.png" alt="die_3" class="inline-icon" style="height:1em;width:auto;"/>
) +p(
<img src="/img/die_4.png" alt="die_4" class="inline-icon" style="height:1em;width:auto;"/>
) + p(
<img src="/img/die_4.png" alt="die_4" class="inline-icon" style="height:1em;width:auto;"/>
) + p(
<img src="/img/die_5.png" alt="die_5" class="inline-icon" style="height:1em;width:auto;"/>
) + p(
<img src="/img/die_6.png" alt="die_6" class="inline-icon" style="height:1em;width:auto;"/>
) = 1
</code></div>
<p>This constraint captures the idea that at least one of the outcomes must obtain.
And behind this way of mathematically expressing this constraint is the idea that
the chance of one of sequence of mutually exclusive outcomes to occur is the sum
of the chances of these outcomes. That is, what we&rsquo;re saying here is that: the
chance of the roll either being a 1, a 2, a 3, a 4, a 5, or a 6 is 1—it&rsquo;s
absolutely certain that at least one outcome will occur.</p>
<p>Other than that, any distribution of masses over the outcomes defines a
probability. That is, all of the following are perfectly fine probability
mass distributions:

<img src="https://logicalmethods.ai/textbook/probability/img/mass_tables.png" class="rounded mx-auto d-block inert-img img-fluid"  width="350px"  alt="" >
</p>
<p>Once we know the chances of the basic outcomes, we can calculate the
probabilities of complex outcomes, like the die showing an even number or
showing a number bigger than four. In the parlance of probability theory, these
are called <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Event_%28probability_theory%29"
  target="_blank">events</a>.
An event is a set of basic outcomes, the basic outcomes which correspond to the
event. So, for example, the event that die shows an even number is:</p>
<div class="customBlock text-center my-4"><code class="customCode">
{
<img src="/img/die_2.png" alt="die_2" class="inline-icon" style="height:1em;width:auto;"/>
, 
<img src="/img/die_4.png" alt="die_4" class="inline-icon" style="height:1em;width:auto;"/>
, 
<img src="/img/die_6.png" alt="die_6" class="inline-icon" style="height:1em;width:auto;"/>
}
</code></div>
Instead, the event that the roll is bigger than four (exclusive) is:
<div class="customBlock text-center my-4"><code class="customCode">
{
<img src="/img/die_5.png" alt="die_5" class="inline-icon" style="height:1em;width:auto;"/>
, 
<img src="/img/die_6.png" alt="die_6" class="inline-icon" style="height:1em;width:auto;"/>
}
</code></div>
<p>To calculate the probability of an event, we simply sum up the probability
masses of its outcomes. So, for example,</p>
<div class="customBlock text-center my-4"><code class="customCode">
Pr({
<img src="/img/die_2.png" alt="die_2" class="inline-icon" style="height:1em;width:auto;"/>
, 
<img src="/img/die_4.png" alt="die_4" class="inline-icon" style="height:1em;width:auto;"/>
, 
<img src="/img/die_6.png" alt="die_6" class="inline-icon" style="height:1em;width:auto;"/>
}) = p(
<img src="/img/die_2.png" alt="die_2" class="inline-icon" style="height:1em;width:auto;"/>
) + p(
<img src="/img/die_4.png" alt="die_4" class="inline-icon" style="height:1em;width:auto;"/>
) + p(
<img src="/img/die_6.png" alt="die_6" class="inline-icon" style="height:1em;width:auto;"/>
)
</code></div>
<p>This probability, then, will differ for each mass function. For our fair die,
e.g., we get <code class="customCode">Pr({
<img src="/img/die_2.png" alt="die_2" class="inline-icon" style="height:1em;width:auto;"/>
, 
<img src="/img/die_4.png" alt="die_4" class="inline-icon" style="height:1em;width:auto;"/>
, 
<img src="/img/die_6.png" alt="die_6" class="inline-icon" style="height:1em;width:auto;"/>
}) = 1/6 + 1/6 + 1/6 =
3/6 = 1/2</code>. But for the unfair distribution, we get <code class="customCode">Pr({
<img src="/img/die_2.png" alt="die_2" class="inline-icon" style="height:1em;width:auto;"/>
, 
<img src="/img/die_4.png" alt="die_4" class="inline-icon" style="height:1em;width:auto;"/>
, 
<img src="/img/die_6.png" alt="die_6" class="inline-icon" style="height:1em;width:auto;"/>
}) = 1/10 + 1/10 + 1/10 = 3/10</code>. Since each outcome <span class="excalifont">ω 
<img src="/img/in.png" alt="Element" class="inline-icon" style="height:1em;width:auto;"/>
Ω</span> corresponds to a singleton event, viz. <code class="customCode">{ω}</code>, we can also write things
like <code class="customCode">Pr(
<img src="/img/die_1.png" alt="die_1" class="inline-icon" style="height:1em;width:auto;"/>
)</code>, which technically would be <code class="customCode">Pr({
<img src="/img/die_1.png" alt="die_1" class="inline-icon" style="height:1em;width:auto;"/>
}) = p(
<img src="/img/die_1.png" alt="die_1" class="inline-icon" style="height:1em;width:auto;"/>
)</code>. More generally, the formula for the probability of an arbitrary
event <code class="customCode">X 
<img src="/img/subseteq.png" alt="Subseteq" class="inline-icon" style="height:1em;width:auto;"/>
 Ω</code> is:

<img src="https://logicalmethods.ai/textbook/probability/img/probability.png" class="rounded mx-auto d-block inert-img img-fluid"  width="200px"  alt="" >
</p>
<p>This is, in a nutshell, the standard model of <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Probability_distribution#Discrete_probability_distribution"
  target="_blank">(finite) discrete
probabilities</a>.
Things get a bit more complicated if we want to allow for non-discrete
<i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Probability_distribution#Absolutely_continuous_probability_distribution"
  target="_blank">continuous</a>
values: where the outcomes cannot be counted like 1, 2, 3, &hellip; but are things
like real-valued functions with values in an interval <span class="excalifont">[0, 1]</span>. But for the
purposes of basic AI-applications, discrete probability theory is more than
enough.</p>
<p>There is a big debate on the foundations of probability theory concerning what
probabilities <em>are</em>:</p>
<ul>
<li>
<p>for <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Bayesian_probability"
  target="_blank">Bayesians</a> probabilities
are a measure of one&rsquo;s degree of belief in something happening;</p>
</li>
<li>
<p>for <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Frequentist_probability"
  target="_blank">frequentists</a>,
probabilities measure how often an event would occur if we&rsquo;d repeat the
experiment infinitely many times;</p>
</li>
<li>
<p>for <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Propensity_probability"
  target="_blank">objectivists</a>
probability is something &ldquo;in the world&rdquo;: just like there&rsquo;s the mass of an
object, there&rsquo;s the chance of it behaving a certain way.</p>
</li>
</ul>
<p>For our purposes, however, the interpretation of probabilities is not crucial.
To explore the relation between probabilities and inductive inference in AI, we
can take a naive view of probabilities as chances or likelihoods of something
happening.</p>
<p>Note that the outcomes of a random experiment are nothing different than the
reasoning scenarios we&rsquo;ve discussed on logical semantics. Suppose, for example,
that rather than a die roll, our random experiment is given by tomorrow&rsquo;s
weather with respect to sun and rain. In this setup, there are four possible
outcomes:

<img src="https://logicalmethods.ai/textbook/probability/img/weather_space.png" class="rounded mx-auto d-block inert-img img-fluid"  width="500px"  alt="" >
</p>
<p>But these are just the models for the propositional language with <span class="excalifont">SUN</span> and
<span class="excalifont">RAIN</span>. In fact, we can think of a probability mass function as an assignment of
values between <code class="customCode">0</code> and <code class="customCode">1</code> to the models of this language. A convenient way of
displaying them is by means of a <strong>probabilistic truth-table</strong>, which next to
the truth-values, gives the probability of a given row. Here&rsquo;s, for example, two
probabilistic truth-tables: one table that represents a situation where we don&rsquo;t know
what&rsquo;ll happen tomorrow, where each outcome is equally likely, and one table
that represents a situation where we know that the sun will shine but it&rsquo;s
uncertain whether it will rain or not:

<img src="https://logicalmethods.ai/textbook/probability/img/prob_tables.png" class="rounded mx-auto d-block inert-img img-fluid"  width="700px"  alt="" >
</p>
<p>Given a probability mass function over the models of a language, we can
calculate the probabilities of arbitrary formulas by calculating the probability
of the proposition they express. Remember that the proposition
<span class="excalifont">
<img src="/img/llbracket.png" alt="Left semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-right:-0.4em"/>
A
<img src="/img/rrbracket.png" alt="Right semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-left:-0.4em"/>
</span> expressed by a formula <span class="excalifont">A</span> is simply the
set of models where the formula is true:</p>
<div class="text-center my-4 excalifont">
<img src="/img/llbracket.png" alt="Left semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-right:-0.4em"/>
A
<img src="/img/rrbracket.png" alt="Right semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-left:-0.4em"/>
 = { v : v(A) = 1 }</div>
But in probabilistic terms, such a proposition is just an event over the sample
space of valuations or models. So, the rules from before tell us that:

<img src="https://logicalmethods.ai/textbook/probability/img/prop_prob.png" class="rounded mx-auto d-block inert-img img-fluid"  width="200px"  alt="" >

<p>That is, to determine the probability of a formula given a probability mass
function, we sum up the masses of all the valuations under which the formula is
true.</p>
<p>Truth-tables are of great help here: if you have a probabilistic
truth-table—like the ones above—you can simply calculate the values of your
formula in question for each row, and then sum up the weights of the rows where
the outcome is <code class="customCode">1</code>. Here&rsquo;s how this works out for our two examples and the
formula <span class="excalifont">SUN
<img src="/img/disjunction.png" alt="Disjunction" class="inline-icon" style="height:1em;width:auto;"/>

<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
RAIN</span>:

<img src="https://logicalmethods.ai/textbook/probability/img/prob_tables_calc.png" class="rounded mx-auto d-block inert-img img-fluid"  width="700px"  alt="" >

In this way, given a probability mass, we can calculate the probability of each
formula.</p>
<p>There&rsquo;s a crucial difference between truth and probabilities, which is
important to pay attention to. The truth-values of complex formulas are
calculated
<i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Recursion_%28computer_science%29"
  target="_blank">recursively</a>,
which means step-by-step from the values of their parts. The <em>probabilities</em> of
complex formulas, however, are <em>not</em> (in general) recursive. Take, for example,
the probability of <span class="excalifont">SUN
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
RAIN</span> under the ignorance distribution and compare
it to the probabilities of <span class="excalifont">SUN</span> and <span class="excalifont">RAIN</span>:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/prob_conjunction_id.png" class="rounded mx-auto d-block inert-img img-fluid"  width="500px"  alt="" >

<p>With this probability mass distribution, we have that <div class="text-center my-4 excalifont">Pr(SUN
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
RAIN) = PR(SUN) x Pr(RAIN)</div></p>
<p>But this formula doesn&rsquo;t apply under all distributions. Look, for example, at
the following table with the distribution <span class="excalifont">p*</span>:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/table_indie.png" class="rounded mx-auto d-block inert-img img-fluid"  width="400px"  alt="" >

<p>This table represents a situation where it&rsquo;s more likely to be sunny than not. We have:</p>
<div class="text-center my-4 excalifont">Pr(SUN) = 1/2 + 1/4 = 3/4</div>
<p>If we&rsquo;re looking at the situations where the sun shines, it&rsquo;s relatively
unlikely that it will rain: the world where the sun shines and it&rsquo;s not raining
has probability <span class="excalifont">1/2</span> and the one where the sun shines and it&rsquo;s raining has
probability <span class="excalifont">1/4</span>.</p>
<p>If instead, we look at the two situations where the sun
isn&rsquo;t shining, we see that it&rsquo;s equally likely among those that it&rsquo;s raining:
both scenarios, sun and rain as well as sun and no rain, have probability
<span class="excalifont">1/8</span>. The probability that it&rsquo;s raining is therefore:</p>
<div class="text-center my-4 excalifont">Pr(RAIN) = 1/8 + 1/4 = 3/8</div>
<p>But if we look at the probability of the conjunction, we get that:</p>
<div class="text-center my-4 excalifont">Pr(SUN
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
RAIN) = 1/4</div>
<p>And clearly, <span class="excalifont">1/4</span> is different from <span class="excalifont">3/4 x 3/8 = 9/32</span>. In fact, there is <em>no</em>
formula that allows us to calculate the probability of a conjunction purely on
the basis of the probabilities of its conjuncts.</p>
<p>What&rsquo;s going on here has to do with probabilistic
<i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Independence_%28probability_theory%29"
  target="_blank">independence</a>.
In the first distribution, the two facts that it&rsquo;s raining and that the sun is
shining are <em>independent</em> of each other: under the distribution, knowing that
the sun is shining, this doesn&rsquo;t have any influence on the probability of it raining.</p>
<p>To further illustrate this, we can use the concept of <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Conditional_probability"
  target="_blank">conditional
probabilities</a>.
We&rsquo;ve already seen the definition of conditional probabilities in terms
of propositions when discussing valid inference:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/cond_prob_sets.png" class="rounded mx-auto d-block inert-img img-fluid"  width="300px"  alt="" >

<p>Here, the crucial condition that <span class="excalifont">Pr(
<img src="/img/llbracket.png" alt="Left semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-right:-0.4em"/>
B
<img src="/img/rrbracket.png" alt="Right semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-left:-0.4em"/>
) ≠ 0</span> applies. But using the following identity:</p>
<div class="text-center my-4 excalifont">
<img src="/img/llbracket.png" alt="Left semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-right:-0.4em"/>
A
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
B 
<img src="/img/rrbracket.png" alt="Right semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-left:-0.4em"/>
 = 
<img src="/img/llbracket.png" alt="Left semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-right:-0.4em"/>
A 
<img src="/img/rrbracket.png" alt="Right semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-left:-0.4em"/>

<img src="/img/cap.png" alt="Intersection" class="inline-icon" style="height:1em;width:auto;"/>

<img src="/img/llbracket.png" alt="Left semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-right:-0.4em"/>
B 
<img src="/img/rrbracket.png" alt="Right semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-left:-0.4em"/>
 </div>
we can also directly define conditional probabilities on formulas:

<img src="https://logicalmethods.ai/textbook/probability/img/conditional_prob.png" class="rounded mx-auto d-block inert-img img-fluid"  width="300px"  alt="" >

<p>Now look at what happens in the case of our ignorance distribution, if we
calculate <span class="excalifont">Pr(RAIN | SUN)</span>. First, we note that: <div class="text-center my-4 excalifont">Pr(RAIN) = 1/4 + 1/4 = 1/2
  Pr(SUN) = 1/4 + 1/4 = 1/2</div></p>
<div class="text-center my-4 excalifont">Pr(SUN 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
RAIN) = 1/4</div>
Now, let's calculate <span class="excalifont">Pr(RAIN | SUN)</span>. We get:
<div class="text-center my-4 excalifont">Pr(RAIN | SUN) = Pr(SUN 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
RAIN)/Pr(SUN) = (1/4)/(1/2) = 2/4 = 1/2</div>
That is, we have that <div class="text-center my-4 excalifont">Pr( RAIN | SUN ) = Pr(RAIN)</div>
In other words, the information that it's sunny doesn't tell us anything about the rain.
<p>If we look at the distribution <span class="excalifont">p*</span>, instead, we get:</p>
<div class="text-center my-4 excalifont">Pr(SUN) = 3/4 &emsp; Pr(RAIN) = 3/8</div>
<div class="text-center my-4 excalifont">Pr(SUN 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
RAIN) = 1/4</div>
<p>And so, we have that:</p>
<div class="text-center my-4 excalifont">Pr(RAIN | SUN) = Pr(SUN 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
RAIN)/Pr(SUN) = (1/4)/(3/4) = 4/12 = 1/3</div>
<p>That means that under the hypothesis that it&rsquo;s sunny, the probability of rain
<em>changes</em>—in fact, it goes down (since <span class="excalifont">1/3 &lt; 3/8</span>).</p>
<p>Two formulas <span class="excalifont">A</span> and <span class="excalifont">B</span> are said to be <strong>probabilistically independent</strong> just
in case if they are like <span class="excalifont">SUN</span> and <span class="excalifont">RAIN</span> in our first distribution, that is
just in case <div class="text-center my-4 excalifont">Pr(A | B) = Pr(A)</div></p>
<p>In the case where <span class="excalifont">A</span> and <span class="excalifont">B</span> are probabilistically independent, we have
the <div class="text-center my-4 excalifont">Pr( A 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
B ) = Pr(A) x Pr(B)</div> To see this, suppose that
<span class="excalifont">Pr(A | B) = Pr(A)</span>. By the formula for conditional probabilities, we
have that <span class="excalifont">Pr(A | B ) = Pr(A 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
B)/Pr(B)</span>. So, if we multiply
by <span class="excalifont">Pr(B)</span>, we get <span class="excalifont">Pr(A | B) x Pr(B) = Pr(A 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
B)</span>. But since
<span class="excalifont">Pr(A | B) = Pr(A)</span> by assumption, we have that <span class="excalifont">Pr(A) x Pr(B) = Pr(A

<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
B)</span>.</p>
<p>In fact, this is a <em>test</em> for probabilistic independence as well: if
<span class="excalifont">Pr(A 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
B) = Pr(A) x Pr(B)</span>, then  <span class="excalifont">Pr(A | B ) = Pr(A)</span>. This
is simply because <span class="excalifont">Pr(A | B) = Pr(A 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
B)/Pr(B)</span>. So, if <span class="excalifont">Pr(A

<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
B) = Pr(A) x Pr(B)</span>, we have that: <div class="text-center my-4 excalifont">Pr(A | B) = Pr(A 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
B)/Pr(B) = (Pr(A) x Pr(B))/Pr(B)= Pr(A)</div></p>
<p>In the absence of independence, the best thing we can say about the probability
of a conjunction is that:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/conjunction.png" class="rounded mx-auto d-block inert-img img-fluid"  width="400px"  alt="" >

<p>This covers conjunction. Disjunction is subject to similar considerations. Take
our ignorance table and look at the disjunction <span class="excalifont">SUN 
<img src="/img/disjunction.png" alt="Disjunction" class="inline-icon" style="height:1em;width:auto;"/>
RAIN</span>:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/disjunction_table.png" class="rounded mx-auto d-block inert-img img-fluid"  width="300px"  alt="" >

<p>We get:</p>
<div class="text-center my-4 excalifont">Pr(SUN 
<img src="/img/disjunction.png" alt="Disjunction" class="inline-icon" style="height:1em;width:auto;"/>
RAIN) = 1/4 + 1/4 + 1/4 = 3/4</div>
<p>Looking at this calculation, you can note that we&rsquo;re adding <span class="excalifont">Pr(SUN), Pr(RAIN),</span>
<em>and</em> <span class="excalifont">Pr(SUN
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
RAIN)</span> to obtain the probability of <span class="excalifont">SUN
<img src="/img/disjunction.png" alt="Disjunction" class="inline-icon" style="height:1em;width:auto;"/>

RAIN</span>.</p>
<p>In fact, this is the <em>general</em> formula for calculating disjunctive
probabilities:</p>
<div class="text-center my-4 excalifont">Pr(A 
<img src="/img/disjunction.png" alt="Disjunction" class="inline-icon" style="height:1em;width:auto;"/>
B) = Pr(A) + Pr(B) + Pr(A 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
B)</div>
<p>But that means that, in general, we cannot calculate the probability of a
disjunction from the probabilities of it&rsquo;s disjuncts—we also need to know the
probability of their conjunction.</p>
<p>Only in the very special case where <span class="excalifont">Pr(A 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
B) = 0</span>, when <span class="excalifont">A</span> and <span class="excalifont">B</span>
are probabilistically <em>incompatible</em>, we get the recursive formula: <div class="text-center my-4 excalifont">Pr(A 
<img src="/img/disjunction.png" alt="Disjunction" class="inline-icon" style="height:1em;width:auto;"/>
B) = Pr(A) + Pr(B)</div></p>
<p>The only case where we have a full recursive rule is the case of negation. Under
each distribution, we have <div class="text-center my-4 excalifont">Pr(
<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
A) = 1 - Pr(A)</div> This follows from
the simple fact that the rows where <span class="excalifont">A</span> is true are precisely the rows where
<span class="excalifont">
<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
A</span> is not, and the masses of the rows add up to <span class="excalifont">1</span>.</p>
<h2 id="probability-laws">Probability Laws<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h2>
<hr>

<p>
<img src="https://logicalmethods.ai/textbook/probability/img/laws.png" class="rounded  float-start inert-img img-fluid m-2"  width="250px"  alt="" >
 The fact that probabilities aren&rsquo;t recursive puts special
emphasis on the <em>laws</em> of probability. Moreover, in AI research, you&rsquo;re often
dealing with situations where its practically
<i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Computational_complexity_theory#tractable_problem"
  target="_blank">intractable</a>
to go through all possible models of the premises and assign them probability masses.
Instead, you&rsquo;ll be <em>estimating</em> the relevant probabilities of the formulas
directly. And when you do that, you have to make sure you&rsquo;re doing this in
accordance with the laws of probability.</p>
<p>The standard axiomatization of probability is due to <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov"
  target="_blank">Andrey
Komogorov</a> and correspondingly
known as the <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Probability_axioms"
  target="_blank">Kolmogorov
axioms</a>. We can formula these
axioms in logical terms and directly in terms of events. First, the logical
axiomatization. It states that for each assignment of probabilities <span class="excalifont">Pr</span> to
formulas in a language, the following laws apply:</p>
<ol>
<li><span class="excalifont">Pr(A) ≥ 0</span>, for all formulas <span class="excalifont">A</span>.</li>
<li><span class="excalifont">Pr(A) = 1</span>, if <span class="excalifont">A</span> is a tautology, that is: <span class="excalifont">
<img src="/img/vDash.png" alt="vDash" class="inline-icon" style="height:1em;width:auto;"/>
 A</span>.</li>
<li><span class="excalifont">Pr(A 
<img src="/img/disjunction.png" alt="Disjunction" class="inline-icon" style="height:1em;width:auto;"/>
 B) = Pr(A) + Pr(B)</span>, given that <span class="excalifont">A</span> and <span class="excalifont">B</span> are logically
incompatible, that is: <span class="excalifont">
<img src="/img/vDash.png" alt="vDash" class="inline-icon" style="height:1em;width:auto;"/>
 A
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>

<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
 B</span>.</li>
</ol>
<p>As you can see, these axioms are rather minimal. But it turns out that they are
<em>sound and complete</em> with respect to the probabilities we&rsquo;ve defined in terms of
probability mass distributions over valuations. That is, every law about
probabilities that holds for all probabilities <span class="excalifont">Pr</span> defined in terms of
probability mass distributions is derivable form these laws, and everything
that&rsquo;s derivable holds for all distributions.</p>
<p>For example, here&rsquo;s how we derive the law of negation:</p>
<div class="text-center my-4 excalifont">Pr(
<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
A) = 1 - Pr(A) ("Negation")</div>
<ul>
<li><span class="excalifont">Pr(A 
<img src="/img/disjunction.png" alt="Disjunction" class="inline-icon" style="height:1em;width:auto;"/>

<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
A) = 1</span>, by axiom 1. since <span class="excalifont">
<img src="/img/vDash.png" alt="vDash" class="inline-icon" style="height:1em;width:auto;"/>
A
<img src="/img/disjunction.png" alt="Disjunction" class="inline-icon" style="height:1em;width:auto;"/>

<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
A</span>.</li>
<li><span class="excalifont">Pr(A 
<img src="/img/disjunction.png" alt="Disjunction" class="inline-icon" style="height:1em;width:auto;"/>

<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
A) = Pr(A) + Pr(
<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
A)</span> by axiom 3. since
<span class="excalifont">
<img src="/img/vDash.png" alt="vDash" class="inline-icon" style="height:1em;width:auto;"/>

<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
(A 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>

<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
A)</span>.</li>
<li>It follows that <span class="excalifont">1 = Pr(A) + Pr(
<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
A)</span>.</li>
<li>But that gives us: <span class="excalifont">Pr(
<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
A) = 1 - Pr(A)</span></li>
</ul>
<p>From this it immediately follows that <span class="excalifont">Pr(A) 
<img src="/img/leq.png" alt="Less than or equal" class="inline-icon" style="height:1em;width:auto;"/>
 1</span> for all <span class="excalifont">A</span>, since <span class="excalifont">Pr(A) = 1</p>
<ul>
<li>Pr(
<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
A)</span>.</li>
</ul>
<p>Or, we can derive that if <span class="excalifont">A
<img src="/img/vDash.png" alt="vDash" class="inline-icon" style="height:1em;width:auto;"/>
B</span>, then <span class="excalifont">Pr(A) 
<img src="/img/leq.png" alt="Less than or equal" class="inline-icon" style="height:1em;width:auto;"/>
 Pr(B)</span>:</p>
<ul>
<li><span class="excalifont">Pr(A 
<img src="/img/disjunction.png" alt="Disjunction" class="inline-icon" style="height:1em;width:auto;"/>
 
 <img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
B) = Pr(A) + Pr(
<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
B)</span>, by axiom 3.,
since from the assumption that <span class="excalifont">A
<img src="/img/vDash.png" alt="vDash" class="inline-icon" style="height:1em;width:auto;"/>
B</span> it follows that
<span class="excalifont">
<img src="/img/vDash.png" alt="vDash" class="inline-icon" style="height:1em;width:auto;"/>

<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
A
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>

<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
B</span>—that is if <span class="excalifont">A</span> and
<span class="excalifont">
<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
B</span> is unsatisfiable.</li>
<li>Since <span class="excalifont">Pr(
<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
B) = 1 - Pr(B)</span>, we have <span class="excalifont">Pr(A 
<img src="/img/disjunction.png" alt="Disjunction" class="inline-icon" style="height:1em;width:auto;"/>
 
 <img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
B) = Pr(A) + (1 - Pr(B))</span>.</li>
<li>So, we get that <span class="excalifont">Pr(B) = Pr(A) + (1 - Pr(A 
<img src="/img/disjunction.png" alt="Disjunction" class="inline-icon" style="height:1em;width:auto;"/>

<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
 B))</span>.</li>
<li>But we know that <span class="excalifont">Pr(A
<img src="/img/disjunction.png" alt="Disjunction" class="inline-icon" style="height:1em;width:auto;"/>

<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
B) 
<img src="/img/leq.png" alt="Less than or equal" class="inline-icon" style="height:1em;width:auto;"/>
 1</span>, so
<span class="excalifont">1 - Pr(A 
<img src="/img/disjunction.png" alt="Disjunction" class="inline-icon" style="height:1em;width:auto;"/>

<img src="/img/negation.png" alt="Negation" class="inline-icon" style="height:1em;width:auto;"/>
 B)</span> is positive and so <span class="excalifont">Pr(B) = Pr(A) + x</span>,
for some positive <span class="excalifont">x</span>.</li>
<li>In other words, <span class="excalifont">Pr(A) 
<img src="/img/leq.png" alt="Less than or equal" class="inline-icon" style="height:1em;width:auto;"/>
 Pr(B)</span>.</li>
</ul>
<p>We can give these axioms completely equivalently directly in terms</p>
<ol>
<li><span class="excalifont">0
<img src="/img/leq.png" alt="Less than or equal" class="inline-icon" style="height:1em;width:auto;"/>
Pr(X)</span>, for all events <span class="excalifont">X 
<img src="/img/subseteq.png" alt="Subseteq" class="inline-icon" style="height:1em;width:auto;"/>
 Ω</span></li>
<li><span class="excalifont">Pr(Ω) = 1</span></li>
<li><span class="excalifont">Pr(X 
<img src="/img/cup.png" alt="Union" class="inline-icon" style="height:1em;width:auto;"/>
Y) = Pr(X) + Pr(Y)</span>, given that <span class="excalifont">X
<img src="/img/cap.png" alt="Intersection" class="inline-icon" style="height:1em;width:auto;"/>
Y = ∅</span>.</li>
</ol>
<p>This axiomatization says precisely the same thing as the previous one once we
realize that <span class="excalifont">
<img src="/img/llbracket.png" alt="Left semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-right:-0.4em"/>
A
<img src="/img/rrbracket.png" alt="Right semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-left:-0.4em"/>
 = Ω</span> means that <span class="excalifont">A</span> is a
logical truth and</p>
<div class="text-center my-4 excalifont">
<img src="/img/llbracket.png" alt="Left semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-right:-0.4em"/>
A 
<img src="/img/disjunction.png" alt="Disjunction" class="inline-icon" style="height:1em;width:auto;"/>
B
<img src="/img/rrbracket.png" alt="Right semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-left:-0.4em"/>
 = 
<img src="/img/llbracket.png" alt="Left semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-right:-0.4em"/>
A 
<img src="/img/rrbracket.png" alt="Right semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-left:-0.4em"/>

<img src="/img/cup.png" alt="Union" class="inline-icon" style="height:1em;width:auto;"/>

<img src="/img/llbracket.png" alt="Left semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-right:-0.4em"/>
B 
<img src="/img/rrbracket.png" alt="Right semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-left:-0.4em"/>
</div>
<h2 id="naive-bayes-classifiers">Naive Bayes Classifiers<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h2>
<hr>

<p>To see how AI-technologies use probabilities for inductive inferences, let&rsquo;s
look at an example: <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Email_filtering"
  target="_blank">spam
filtering</a> with <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier"
  target="_blank">naive Bayes
classifiers</a>.</p>
<p>For our example, let&rsquo;s suppose that <span class="code" style="margin-right:-4px;vertical-align: text-top;transform-origin: 50%  50%;transform:rotate(180deg);display:inline-block">IA</span>
 receives an
email:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/spam_msg.png" class="rounded mx-auto d-block inert-img img-fluid"  width="500px"  alt="" >

<p>How can <span class="code" style="margin-right:-4px;vertical-align: text-top;transform-origin: 50%  50%;transform:rotate(180deg);display:inline-block">IA</span>
 tell that this is a <code class="customCode">SPAM</code> email?</p>
<p>Since we&rsquo;re in a logic course, you might think that we&rsquo;d try to develop an
expert system for this task. We devise a propositional language with atoms like</p>
<div class="customBlock text-center my-4"><code class="customCode">
UnknownSender, SubjectUrgent, ExclamationMarks, MoneyTalk, SPAM
</code></div>
<p>In this language, we can formula conditional rules like:</p>
<ul>
<li><code class="customCode">(SubjectUrgent
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
MoneyTalk) 
<img src="/img/to.png" alt="Implication" class="inline-icon" style="height:1em;width:auto;"/>
 SPAM</code></li>
<li><code class="customCode">(ExlamationMarks
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
SubjectUrgent) 
<img src="/img/to.png" alt="Implication" class="inline-icon" style="height:1em;width:auto;"/>
 SPAM</code></li>
<li><code class="customCode">&hellip;</code></li>
</ul>
<p>Then we could run a filter through each email and check whether the propositions
<code class="customCode">UnknownSender, SubjectUrgent, ExclamationMarks, MoneyTalk</code> are true and use the
KB to see if we can derive <code class="customCode">SPAM</code> using methods like forward chaining or
backward chaining or resolution.</p>
<p>But the problem with this approach is that none of these indicators are 100%
reliable. We can have an email that satisfies all of them but is perfectly
harmless:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/ai_non_spam.png" class="rounded mx-auto d-block inert-img img-fluid"  width="500px"  alt="" >

<p>That is, using deductive inference will lead to a lot of <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/False_positives_and_false_negatives"
  target="_blank">false
positives</a>.
In fact, the conditions that we&rsquo;ve mentioned aren&rsquo;t <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Necessity_and_sufficiency"
  target="_blank">logically
sufficient</a> for an
email being <code class="customCode">SPAM</code>, they are fallible
<i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Evidence"
  target="_blank"><em>evidence</em></a> for it.</p>
<p>A powerful approach to inductive inference uses probabilities and <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Bayes%27_theorem"
  target="_blank">Bayes'
rule</a>. The basic idea is that we
can say that a piece of propositional evidence, <span class="excalifont">E</span>, confirms a hypothesis, <span class="excalifont">H</span>,
just in case: <div class="text-center my-4 excalifont">Pr(H | E) &gt; Pr(H)</div> That is, the evidence confirms the hypothesis
just in case the probability of the hypothesis goes up if we assume the
evidence. In other words, evidence is a form of inductive inference.</p>
<p>So, what we&rsquo;re looking for is <span class="excalifont">Pr(<code class="customCode">SPAM</code> | <code class="customCode">[MARKERS]</code>)</span>, where <span class="excalifont"><code class="customCode">[MARKERS]</code></span>
is a combination of things like <code class="customCode">UnknownSender, SubjectUrgent, ExclamationMarks,
MoneyTalk</code>. That is see whether the present markers are evidence for the mail being
<code class="customCode">SPAM</code>, we need to calculate <span class="excalifont">Pr(<code class="customCode">SPAM</code> | <code class="customCode">[MARKERS]</code>)</span> as well as <span class="excalifont">Pr(<code class="customCode">SPAM</code>)</span>.</p>
<p>Bayes rule is, in essence, a convenient way of calculating <span class="excalifont">Pr(<code class="customCode">SPAM</code> | <code class="customCode">[MARKERS]</code>)</span>:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/bayes_rule.png" class="rounded mx-auto d-block inert-img img-fluid"  width="500px"  alt="" >

<p>This calculation is derived by simply substituting <span class="excalifont">P(H 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
E)</span> with
<span class="excalifont">Pr(H) x Pr(E | H)</span> from the general rule for conjunction. The terms in the
equation have suggestive names:</p>
<ul>
<li>
<p><span class="excalifont">Pr(H | E)</span> is known as the <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Posterior_probability"
  target="_blank"><em>posterior</em> probability</a> of the hypothesis given the evidence.</p>
</li>
<li>
<p><span class="excalifont">Pr(H)</span> is called the <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Prior_probability"
  target="_blank"><em>prior</em> probability</a> of the hypothesis, independent of the
evidence.</p>
</li>
<li>
<p><span class="excalifont">Pr(E | H)</span> is known as the
<i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Likelihood_function"
  target="_blank"><em>likelihood</em></a> of the
evidence given the hypothesis.</p>
</li>
<li>
<p><span class="excalifont">Pr(E)</span> is known as the <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Marginal_likelihood"
  target="_blank">marginal
likelihood</a> of the evidence.</p>
</li>
</ul>
<p>Plugging in <code class="customCode">SPAM</code> and <code class="customCode">[MARKERS]</code>, we obtain:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/bayes_applied.png" class="rounded mx-auto d-block inert-img img-fluid"  width="500px"  alt="" >

<p>This is progress: the prior probability, <span class="excalifont">Pr(<code class="customCode">SPAM</code>)</span>, we can estimate using the
<i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Email_spam"
  target="_blank">frequency of SPAM</a>. For this example,
let&rsquo;s estimate it optimistically as 20%. In probabilistic terms:</p>
<div class="customBlock text-center my-4"><code class="customCode">
Pr(SPAM) = 0.2
</code></div>
<p>For individual markers, like <code class="customCode">UnknownSender, SubjectUrgent, ExclamationMarks,
MoneyTalk</code>, we have to estimate the marginal likelihood <span class="excalifont">Pr(<code class="customCode">[Markers]</code>)</span> and
likelihood <span class="excalifont">Pr(<code class="customCode">[Markers]</code> | <code class="customCode">SPAM</code>)</span>. In practice, this happens on the basis of
a frequency analysis of datasets of emails, especially the ones <em>you</em> have
received. For each marker, we can easily check how often it occurs in an email:
how many emails are from unknown sender, how many emails have &ldquo;urgent&rdquo; in the
subject line, and so on. Dividing these numbers by the total number of emails
gives us a decent estimate of their respective probabilities. We might, for
example, find:</p>
<div class="customBlock text-center my-4"><code class="customCode">
Pr(UnknownSender)  = 0.3
</code></div>
<div class="customBlock text-center my-4"><code class="customCode">
Pr(SubjectUrgent)  = 0.4
</code></div>
<div class="customBlock text-center my-4"><code class="customCode">
Pr(ExclamationMarks) = 0.6
</code></div>
<div class="customBlock text-center my-4"><code class="customCode">
Pr(MoneyTalk) = 0.3
</code></div>
<p>The same procedure, we can use to estimate the likelihoods for the markers given
in <code class="customCode">SPAM</code> messages. What&rsquo;s the frequency of SPAM emails with an unknown sender,
SPAM emails with urgent subject line, etc. For example, we might find:</p>
<div class="customBlock text-center my-4"><code class="customCode">
Pr( UnknownSender | SPAM )  = 0.4
</code></div>
<div class="customBlock text-center my-4"><code class="customCode">
Pr( SubjectUrgent | SPAM )  = 0.6
</code></div>
<div class="customBlock text-center my-4"><code class="customCode">
Pr( ExclamationMarks | SPAM ) = 0.7
</code></div>
<div class="customBlock text-center my-4"><code class="customCode">
Pr( MoneyTalk | SPAM) = 0.6
</code></div>
<p>We can plug this data into Bayes rule and obtain estimates of how likely it is
that an email is <code class="customCode">SPAM</code> given that it comes from an unknown sender:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/unknown_sender.png" class="rounded mx-auto d-block inert-img img-fluid"  width="500px"  alt="" >

<p>Since</p>
<div class="customBlock text-center my-4"><code class="customCode">
<p>Pr(Spam | UnknownSender) = 0.26 &gt; 0.2 = Pr(Spam)
</code></div></p>
<p>receiving an email from an unknown sender <em>is</em> evidence of it being <code class="customCode">SPAM</code>—but
not very <em>strong</em> evidence.</p>
<p>What does this mean? Well, for one, the probability of the email being <code class="customCode">SPAM</code>
only went up by 0.06. This is the so-called <strong>increase of firmness measure</strong></p>
<div class="customBlock text-center my-4"><code class="customCode">
incOfFirmness(E, H) = |Pr(H | E) - Pr(H)|
</code></div>
<p>In our case, the increase of firmness is not very strong.</p>
<p>But note that the probability of the email being <code class="customCode">SPAM</code> went up from <span class="excalifont">0.2</span> to
<span class="excalifont">0.26</span>, which is a 30% increase. This is known as the <strong>the ratio measure</strong>:</p>
<div class="customBlock text-center my-4"><code class="customCode">
ratioStrength(E, H) = Pr(H | E)/Pr(H)
</code></div>
<p>There are many more such confirmation measures. In fact, in industry
implementations of naive Bayesian classifiers the co-called <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Likelihood-ratio_test"
  target="_blank">log-likelihood
ratio</a> is commonly used,
but we won&rsquo;t go into the more involved mathematical details here.</p>
<p>Different measures have different advantages: for example, the increase of
firmness gives an intuitively clear and robust measure of strength of evidence,
but it doesn&rsquo;t work very well with evidence and hypotheses close to 0 or 1.
The ration measure, instead also works well close to extreme values, but it is
not <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Norm_%28mathematics%29"
  target="_blank">normed</a>—in particular
it&rsquo;s value can get arbitrarily high making comparisons difficult.</p>
<p>But regardless of different measures of <em>how much</em> weight the evidence carries,
there&rsquo;s another fundamental question we&rsquo;ve got to address: if the posterior
probability is &ldquo;only&rdquo; 0.26, it&rsquo;s still more likely that this <em>isn&rsquo;t</em> <code class="customCode">SPAM</code> than
that it is. What we need to do is to set a
<i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Threshold_model"
  target="_blank">threshold</a> for classifying email
as <code class="customCode">SPAM</code>. A natural minimum is to say that <span class="excalifont">Pr(H | E)</span> should <em>at least</em> be
0.5. But if <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/False_positives_and_false_negatives"
  target="_blank">false
positives</a>
are particularly bad—when classifying email as <code class="customCode">SPAM</code> that isn&rsquo;t risks important
messages to be missed—we should set the bar high. Perhaps we want to have 0.75?
0.8? It ultimately depends on the stakes of the situation, but let&rsquo;s try to
reach 0.5 in our <code class="customCode">SPAM</code> filter.</p>
<p>To achieve this, we need to <em>combine</em> different markers. What we <em>want</em> to
do is to combine different <code class="customCode">SPAM</code>-markers. In fact, our example mail had them
all. So, we want to calculate:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/combined_markers.png" class="rounded mx-auto d-block inert-img img-fluid"  width="900px"  alt="" >

<p>But now we have a new kind of probability to estimate: the conjunctive
probabilities of the markers. In fact, if we want our classifier to not only
work in this case, but also when one or more of the markers are absent. But
since conjunctive probabilities are not recursively calculable from their
conjunctions&rsquo; probabilities, this leads to a problem of <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Combinatorial_explosion"
  target="_blank">combinatorial
explosion</a> In our
example, we only have 4 markers, but that&rsquo;s already quite a large number of
conjunctions to estimate: all 4 markers by themselves, their 6 binary
conjunctions (up to re-ordering), their 3 ternary conjunctions, and the
conjunction of all 4 makes 14 conjunctions we need to estimate. This quickly
becomes intractable.</p>
<p>This is where the &ldquo;naive&rdquo; in naive Bayes classifiers comes in. The defining
assumption is that the different markers are probabilistically independent of
each other, both given <code class="customCode">SPAM</code> and not given <code class="customCode">SPAM</code>. That is, we can calculate:</p>
<div class="customBlock text-center my-4"><code class="customCode">
Pr( UnknownSender 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 SubjectUrgent 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 ExclamationMarks 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 MoneyTalk | SPAM)
</code></div>
<div class="customBlock text-center my-4"><code class="customCode">
=
</code></div>
<div class="customBlock text-center my-4"><code class="customCode">
Pr(UnknownSender | SPAM) x Pr(SubjectUrgent | SPAM) x ... 
</code></div>
<div class="customBlock text-center my-4"><code class="customCode">
... x Pr(ExclamationMarks | SPAM) x Pr(MoneyTalk | SPAM)
</code></div>
<p>And similarly,</p>
<div class="customBlock text-center my-4"><code class="customCode">
Pr( UnknownSender 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 SubjectUrgent 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 ExclamationMarks 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 MoneyTalk)
</code></div>
<div class="customBlock text-center my-4"><code class="customCode">
=
</code></div>
<div class="customBlock text-center my-4"><code class="customCode">
Pr(UnknownSender) x Pr(SubjectUrgent) x ... 
</code></div>
<div class="customBlock text-center my-4"><code class="customCode">
... x Pr(ExclamationMarks) x Pr(MoneyTalk)
</code></div>
<p>This, then is the <strong>naive Bayes assumption</strong>: that the different markers are
conditionally independent. It is naive because it is clearly true in strict
terms: whether urgent and exclamation points occur is not independent of each
other. But it turns out that this gives us pretty good <em>classifications</em> in
practice. It gives us for our email:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/spam_calculation.png" class="rounded mx-auto d-block inert-img img-fluid"  width="900px"  alt="" >

<p>A pretty clear verdict, which surpasses any reasonable threshold: we&rsquo;ve got a
strong inductive inference that this is spam, given the observed features.</p>

<img src="https://logicalmethods.ai/textbook/probability/img/spam_verdict.png" class="rounded mx-auto d-block inert-img img-fluid"  width="400px"  alt="" >

<p>But it also allows us to quickly estimate the <code class="customCode">SPAM</code> likelihood of other emails.
For example, if the exclamation marks are absent, the formula would still give a
<code class="customCode">0.8</code> probability of <code class="customCode">SPAM</code> given that the other features are still present. But
if money talk is eliminated, the probability drops down to <code class="customCode">0.46</code>. Fine-tuning
these values, checking them against real cases and learning from data is the
topic of ongoing machine learning research.</p>
<h2 id="inductive-logic">Inductive logic<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h2>
<hr>

<p><code class="customCode">SPAM</code>-classification is a case of <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Material_inference"
  target="_blank"><em>material</em> inductive
inference</a>: what counts as a
good inductive inference depends on the concrete (conditional) probabilities of the premises
and conclusion. Logically valid inductive inference is inductive inference that
purely depends on their logical form. We can cash this out as probability
raising under <em>all</em> probabilities:</p>
<div class="customBlock text-center my-4"><code class="customCode">
P₁, P₂, ... 
<img src="/img/approx.png" alt="approxmodels" class="inline-icon" style="height:1em;width:auto;"/>
 C if and only if for all Pr, we have
Pr(C | P₁ 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 P₂
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 ) ≥ P(C)
</code></div>
<p>Inductive logic is concerned with the study of inductively valid inference patterns.
Today, this is mainly done as statistics and probability theory, whose study
goes beyond the scope of this course. But we can get a rough idea of some core
ideas by returning to one of our first examples of inductive inference: <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Inductive_reasoning#Enumerative_induction"
  target="_blank">enumerative induction</a>.</p>
<p>Take our inference about the marbles:</p>
<div class="text-center my-4 excalifont">All marbles we've observed so far were white
<img src="/img/therefore.png" alt="Therefore" class="inline-icon" style="height:1em;width:auto;"/>
All marbles
are white</div>
<p>If we formalize this inference with a sufficient number of observations, we&rsquo;ll
look at something like:</p>
<div class="customBlock text-center my-4"><code class="customCode">
White m₁, ..., White mₙ 
<img src="/img/therefore.png" alt="Therefore" class="inline-icon" style="height:1em;width:auto;"/>

<img src="/img/forall.png" alt="forall" class="inline-icon" style="height:1em;width:auto;margin-right:-0.4em"/>

x White x
</code></div>
<p>To assess the logically validity of this inference, we need to think about the
conditional probability <em>independently</em> of any concrete probability
distribution:</p>
<div class="customBlock text-center my-4"><code class="customCode">
Pr(
<img src="/img/forall.png" alt="forall" class="inline-icon" style="height:1em;width:auto;margin-right:-0.4em"/>

x White x | White m₁
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 ...
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 White mₙ)
</code></div>
<p>For this purpose, we can make use of probabilistic theorems like the following:</p>
<p><strong>Theorem</strong>: If <code class="customCode">C
<img src="/img/vDash.png" alt="vDash" class="inline-icon" style="height:1em;width:auto;"/>
 P₁ 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 P₂&hellip;</code> (the conclusion
deductively implies the premises), <code class="customCode">Pr(P₁ 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 P₂) ≠ 1</code> (the premises
aren&rsquo;t tautologies), and <code class="customCode">Pr(C) ≠ 0</code> (the conclusion is not impossible), then
<code class="customCode">Pr(C | P₁ 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 P₂&hellip;) &gt; Pr(C)</code>.</p>
<p>To see that this must be true, we need observe the logical fact that:</p>
<div class="customBlock text-center my-4"><code class="customCode">
if C
<img src="/img/vDash.png" alt="vDash" class="inline-icon" style="height:1em;width:auto;"/>
 P₁ 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 P₂, then 
<img src="/img/llbracket.png" alt="Left semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-right:-0.4em"/>
C 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
P₁ 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 P₂
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
...
<img src="/img/rrbracket.png" alt="Right semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-left:-0.4em"/>
 = 
<img src="/img/llbracket.png" alt="Left semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-right:-0.4em"/>
C
<img src="/img/rrbracket.png" alt="Right semantic bracket" class="inline-icon" style="height:1.1em;width:auto;margin-left:-0.4em"/>

</code></div>
<p>Using this, we can calculate:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/enum_calc.png" class="rounded mx-auto d-block inert-img img-fluid"  width="500px"  alt="" >

<p>That is, since <span class="excalifont">Pr(P₁ 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 P₂
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
&hellip;) &lt; 1</span>, we know that
<span class="excalifont">1/Pr(P₁ 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 P₂
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
&hellip;) &gt; 1</span> and so <span class="excalifont">Pr(C | P₁ 
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>

P₂
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
&hellip;)</span> is more than one times <span class="excalifont">Pr(C)</span>—in other words, <span class="excalifont">Pr(C | P₁

<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 P₂
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
&hellip;)</span> is strictly bigger than <span class="excalifont">Pr(C)</span>.</p>
<p>This establishes that enumerative induction is at least weakly inductively
valid: the premises will always raise the likelihood of the conclusion.</p>
<p>The strength of the inference depends on the probability of <span class="excalifont">P₁
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>

P₂
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
&hellip;</span>—the less likely the premises the bigger the factor in the
above equation and, correspondingly, the stronger the inference. This justifies
the condition that we need to sample our premises well—it should be unlikely
that they are jointly true. We can achieve this by finding many independent
instances and calculate <code class="customCode">Pr(P₁
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
 P₂
<img src="/img/conjunction.png" alt="Conjunction" class="inline-icon" style="height:1em;width:auto;"/>
&hellip;) = Pr(P₁) × Pr(P₂)
× &hellip;</code>, which will eventually bring us below any desired threshold. Or we</p>
<p>Inductive reasoning is the ultimate foundation for most machine learning
techniques, but also for simple algorithms like &ldquo;People who liked this
show also liked &hellip;&quot;-style recommendation on streaming platforms, which makes
inductive logic a fundamental tool in an AI-researchers toolbox.</p>


  </div>
   <p><em>Last edited: </em>21/10/2024</p>
</div>


<ul class="navbar-nav flex-row justify-content-between mt-auto">
  
  <li class="nav-item">
    <a href="/textbook/many-valued/" class="btn" tabindex="-1" role="button" aria-disabled="true">
      <i class="bi bi-chevron-left"></i>
    </a>
    
  </li>
  <li class="nav-item">
    <button class="btn btn-back-to-top">
    <i class="bi bi-chevron-double-up"></i>
  </button>
  </li>
  <li class="nav-item">
    
    <a href="/textbook/learning/" class="btn" tabindex="-1" role="button" aria-disabled="true">
      <i class="bi bi-chevron-right"></i>
    </a>
    
  </li>
</ul>

    </main>
    <footer class="align-self-center d-flex flex-column mt-md-1 p-0" data-bs-theme="dark">
      <nav class="navbar bg-dark shadow" data-bs-theme="dark" aria-label="footer-navbar">
  <div class="container-sm align-self-center px-3">
    <a class="navbar-brand fs-6" href="/about" aria-label="copyright-link">&copy; 2025 jkorbmacher et al. </a>
    <ul class="navbar-nav">
      <li class="nav-item">
        <a class="nav-link fs-6" href="https://github.com/jkorb/logicalmethods.ai" target="_blank"><i class="bi bi-github"></i></a>
      </li>
    </ul>
  </div>
</nav>
 
      <script src="https://logicalmethods.ai/js/helpers.js"></script>
      <script src="https://logicalmethods.ai/bootstrap/dist/js/bootstrap.bundle.min.js"></script>
      
      
      
    </footer>
  </body>
</html>
