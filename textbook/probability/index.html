<!doctype html>
<html lang="en" data-bs-theme="light">
  <head>
    <meta charset="utf-8"></meta>
    <meta name="viewport" content="width=device-width, initial-scale=1"></meta>
    <title>logicalmethods.ai &ndash; Logic and probability </title>
    <link rel="icon" type="image/x-icon" href="https://logicalmethods.ai/favicon.ico"></link>
    <link href="https://logicalmethods.ai/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet"></link>
    <link rel="stylesheet" href="https://logicalmethods.ai/icons/font/bootstrap-icons.min.css"></link>
    <link rel="stylesheet" href="https://logicalmethods.ai/css/layout.css"></link>
    <link rel="stylesheet" href="https://logicalmethods.ai/css/syntax_hl.css"></link>
    <link href="https://logicalmethods.ai/css/textbook.css" rel="stylesheet"></link>
    <style>
      
    :root {
      --chapter: "11";
    }

    </style>
    
      <link rel="stylesheet" href="https://logicalmethods.ai/katex/katex.min.css">
<script defer src="https://logicalmethods.ai/katex/katex.min.js"></script>
<script defer src="https://logicalmethods.ai/katex/contrib/auto-render.min.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},     
        {left: '$', right: '$', display: false},     
        {left: '\\[', right: '\\]', display: true},   
        {left: '\\(', right: '\\)', display: false},  
      ],
      throwOnError : false
    });
  });
</script>


    
  </head>
  <body class="bg-black d-flex flex-column m-0 p-0">
    <div class="offcanvas offcanvas-start" tabindex="-1" id="offcanvas-nav" aria-labelledby="offcanvas-navLabel" data-bs-theme="dark" data-bs-backdrop="false">
  <div class="offcanvas-header">
    <div class="h5 offcanvas-title" id="offcanvas-navLabel">
      Browse course
    </div>
    <button type="button" class="btn-close" data-bs-dismiss="offcanvas" aria-label="Close"></button>
  </div>
  
  <div class="offcanvas-body">
    <ul class="navbar-nav">
      
      <li class="nav-item">
        <div class="d-flex flex-row">
          <div class="container">
            <a class="nav-link " href="/about/"><i class="bi-file-person-fill"></i> &nbsp;&nbsp; About</a>
          </div>
      </li>
      
      
      <hr>
      <li class="nav-item">
        <div class="d-flex flex-row">
          <div class="container">
            <a class="nav-link fw-bold active" href="/textbook/"><i class="bi bi-book"></i> &nbsp;&nbsp; Textbook</a>
          </div>
          <button 
            class="btn btn-sm btn-dark" 
            href="/textbook/"
            id="navbarSectiontxt-home" data-bs-toggle="collapse"
            data-bs-target="#collapsetxt-home"  
            aria-expanded="true"
            aria-controls="collapsetxt-home">
            <i class="bi bi-caret-left-fill toggle-icon"></i>
          </button>
        </div>
        <div class="collapse show" id="collapsetxt-home">
          <ul class="list-unstyled ms-3">
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/logic-and-ai/"
              >1. 
                <i class="bi bi-unlock-fill"></i>
                 Logic and AI
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/formal-languages/"
              >2. 
                <i class="bi bi-unlock-fill"></i>
                 Formal languages
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/valid-inference/"
              >3. 
                <i class="bi bi-unlock-fill"></i>
                 Valid Inference
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/boolean/"
              >4. 
                <i class="bi bi-unlock-fill"></i>
                 Boolean algebra
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/sat/"
              >5. 
                <i class="bi bi-unlock-fill"></i>
                 Boolean satisfiability
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/conditionals/"
              >6. 
                <i class="bi bi-unlock-fill"></i>
                 Logical conditionals
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/proofs/"
              >7. 
                <i class="bi bi-unlock-fill"></i>
                 Logical proofs
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/textbook/fol/"
              >8. 
                <i class="bi bi-unlock-fill"></i>
                 FOL
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                link-opacity-50
                "
                href="javascript:;"
              >9. 
                <i class="bi bi-lock-fill"></i>
                 FOL Inference
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                link-opacity-50
                "
                href="javascript:;"
              >10. 
                <i class="bi bi-lock-fill"></i>
                 Many-valued logics
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                fw-bold active
                link-opacity-50
                "
                href="javascript:;"
              >11. 
                <i class="bi bi-lock-fill"></i>
                 Logic and probability
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                link-opacity-50
                "
                href="javascript:;"
              >12. 
                <i class="bi bi-lock-fill"></i>
                 Logic-based learning
              </a>
            </li>
            
          </ul>
        </div>
          
      </li>
      
      
      <hr>
      <li class="nav-item">
        <div class="d-flex flex-row">
          <div class="container">
            <a class="nav-link " href="/slides/"><i class="bi bi-rocket-takeoff-fill"></i> &nbsp;&nbsp; Slides</a>
          </div>
          <button 
            class="btn btn-sm btn-dark" 
            href="/slides/"
            id="navbarSectionsli-home" data-bs-toggle="collapse"
            data-bs-target="#collapsesli-home"  
            aria-expanded="aria-expanded=false"
            aria-controls="collapsesli-home">
            <i class="bi bi-caret-left-fill toggle-icon"></i>
          </button>
        </div>
        <div class="collapse " id="collapsesli-home">
          <ul class="list-unstyled ms-3">
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/teaser/"
              >0. 
                <i class="bi bi-unlock-fill"></i>
                 Preamble
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/logic-and-ai/"
              >1. 
                <i class="bi bi-unlock-fill"></i>
                 Logic and AI
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/formal-languages/"
              >2. 
                <i class="bi bi-unlock-fill"></i>
                 Formal languages
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/valid-inference/"
              >3. 
                <i class="bi bi-unlock-fill"></i>
                 Valid inference
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/boolean/"
              >4. 
                <i class="bi bi-unlock-fill"></i>
                 Boolean algebra
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/sat/"
              >5. 
                <i class="bi bi-unlock-fill"></i>
                 Boolean satisfiability
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/conditionals/"
              >6. 
                <i class="bi bi-unlock-fill"></i>
                 Logical conditionals
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/proof/"
              >7. 
                <i class="bi bi-unlock-fill"></i>
                 Logical proofs
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/slides/fol/"
              >8. 
                <i class="bi bi-unlock-fill"></i>
                 FOL
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                link-opacity-50
                "
                href="javascript:;"
              >9. 
                <i class="bi bi-lock-fill"></i>
                 FOL inference
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                link-opacity-50
                "
                href="javascript:;"
              >10. 
                <i class="bi bi-lock-fill"></i>
                 Many-valued logics
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                link-opacity-50
                "
                href="javascript:;"
              >11. 
                <i class="bi bi-lock-fill"></i>
                 Logic and probability
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                link-opacity-50
                "
                href="javascript:;"
              >12. 
                <i class="bi bi-lock-fill"></i>
                 Logical learning
              </a>
            </li>
            
          </ul>
        </div>
          
      </li>
      
      
      <hr>
      <li class="nav-item">
        <div class="d-flex flex-row">
          <div class="container">
            <a class="nav-link " href="/exercises/"><i class="bi-gear-fill"></i> &nbsp;&nbsp; Exercises</a>
          </div>
          <button 
            class="btn btn-sm btn-dark" 
            href="/exercises/"
            id="navbarSectionexe-home" data-bs-toggle="collapse"
            data-bs-target="#collapseexe-home"  
            aria-expanded="aria-expanded=false"
            aria-controls="collapseexe-home">
            <i class="bi bi-caret-left-fill toggle-icon"></i>
          </button>
        </div>
        <div class="collapse " id="collapseexe-home">
          <ul class="list-unstyled ms-3">
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/preamble/"
              >0. 
                <i class="bi bi-unlock-fill"></i>
                 Preamble
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/logic-and-ai/"
              >1. 
                <i class="bi bi-unlock-fill"></i>
                 Logic and AI
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/formal-languages/"
              >2. 
                <i class="bi bi-unlock-fill"></i>
                 Formal languages
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/valid-inference/"
              >3. 
                <i class="bi bi-unlock-fill"></i>
                 Valid inference
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/boolean/"
              >4. 
                <i class="bi bi-unlock-fill"></i>
                 Boolean algebra
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/sat/"
              >5. 
                <i class="bi bi-unlock-fill"></i>
                 Boolean satisfiability
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/conditionals/"
              >6. 
                <i class="bi bi-unlock-fill"></i>
                 Logical conditionals
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/proof/"
              >7. 
                <i class="bi bi-unlock-fill"></i>
                 Logical proofs
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/exercises/fol/"
              >8. 
                <i class="bi bi-unlock-fill"></i>
                 FOL
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                link-opacity-50
                "
                href="javascript:;"
              >9. 
                <i class="bi bi-lock-fill"></i>
                 FOL inference
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                link-opacity-50
                "
                href="javascript:;"
              >10. 
                <i class="bi bi-lock-fill"></i>
                 Many-valued logics
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                link-opacity-50
                "
                href="javascript:;"
              >11. 
                <i class="bi bi-lock-fill"></i>
                 Logic and probability
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                link-opacity-50
                "
                href="javascript:;"
              >12. 
                <i class="bi bi-lock-fill"></i>
                 Logical learning
              </a>
            </li>
            
          </ul>
        </div>
          
      </li>
      
      
      <hr>
      <li class="nav-item">
        <div class="d-flex flex-row">
          <div class="container">
            <a class="nav-link " href="/assignments/"><i class="bi bi-house-gear-fill"></i> &nbsp;&nbsp; Assignments</a>
          </div>
          <button 
            class="btn btn-sm btn-dark" 
            href="/assignments/"
            id="navbarSectionass-home" data-bs-toggle="collapse"
            data-bs-target="#collapseass-home"  
            aria-expanded="aria-expanded=false"
            aria-controls="collapseass-home">
            <i class="bi bi-caret-left-fill toggle-icon"></i>
          </button>
        </div>
        <div class="collapse " id="collapseass-home">
          <ul class="list-unstyled ms-3">
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/assignments/assignment_1/"
              >1. 
                <i class="bi bi-unlock-fill"></i>
                 Assignment 1 (due 09/19/2025)
              </a>
            </li>
            
            <li>
              <a class="dropdown-item 
                
                
                "
                href="/assignments/assignment_2/"
              >2. 
                <i class="bi bi-unlock-fill"></i>
                 Assignment 2 (due 03/10/2025)
              </a>
            </li>
            
          </ul>
        </div>
          
      </li>
      
      
      
      
      
    </ul>
  </div>
</div>



    <header class="align-self-center d-flex flex-column m-0 mb-md-1 p-0" data-bs-theme="dark">
      <nav class="navbar bg-dark shadow" data-bs-theme="dark" aria-label="header-navbar">
  <div class="container-sm align-self-center px-3">
    <a class="navbar-brand fs-6" href="https://logicalmethods.ai/">
    <img src="https://logicalmethods.ai/img/nav_id.png" class="inert-img img-fluid m-2" draggable="false" width="400px">
    </a>
    <button class="navbar-toggler border-0" type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas-nav" aria-controls="offcanvas-nav">
      <span class="navbar-toggler-icon"></span>
    </button>
  </div>
</nav>

      
      
    </header>
    
    <main class="align-self-center d-flex flex-column flex-fill bg-white m-md-1 p-0" data-bs-theme="light">
      <ul class="navbar-nav flex-row justify-content-center mt-2">

  <li class="nav-item">
    <a href="/textbook/" class="btn" style="font-size: 20pt;" tabindex="-1" role="button" aria-disabled="false">
      <i class="bi bi-house-up"></i>
    </a>
  </li>
</ul>
<div class="container chapter">
  <p>By: <em>Johannes Korbmacher</em></p>
  <div class="m-4"><h1 id="logic-and-probability">Logic and probability<button class="btn btn-back-to-top">
    
  </button>
</h1>
<hr>

<p>In Chapters 


<a href="https://logicalmethods.ai/textbook/logic-and-ai/"
   
   
   
   
   
   > 1. Logic and AI</a>

 and 


<a href="https://logicalmethods.ai/textbook/valid-inference/"
   
   
   
   
   
   > 2. Valid
inference</a>

, we&rsquo;ve distinguished between <strong>deductive</strong> inference
(the truth of the premises guarantees the truth of the conclusion) and
<strong>inductive</strong> inference (the truth of the premises makes the truth of the
conclusion more likely). So far, we&rsquo;ve only focused on models of deductive
inference. In this chapter, you&rsquo;ll learn more about how to model inductive
inference.</p>
<p>We&rsquo;ll begin by looking at how to 
<a class="link-dark" href="#models-of-probability">model probabilities</a>
in a logical setting. We&rsquo;ll then go through a useful method for calculating
probabilities using 
<a class="link-dark" href="#probability-truth-tables">probability truth-tables</a>. After
this, we turn to 
<a class="link-dark" href="#conditional-probabilities">conditional probabilities</a>, which
then allow us to discuss a simple model of 
<a class="link-dark" href="#inductive-validity">inductive
validity</a>. To conclude the chapter, we look at some
examples of valid and invalid inductive inferences in our model.</p>
<h2 id="models-of-probability">Models of probability<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h2>
<hr>

<p>The probability of an event describes the <strong>how likely it is</strong> that the event
occurs. Think, for example, of how likely it is that you roll a 3 on a 6
sided-die, or how likely it is that it rains tomorrow.</p>
<p>What this <em>really</em> means is actually a matter of substantial debate. There
different schools of thought on the interpretation of probability:</p>
<ul>
<li>
<p>Objectivists argue that the likelihood we&rsquo;re talking about here is an
objective feature of reality, sometimes called <em>objective chance</em>. An example of
this are <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Frequentist_probability"
  target="_blank">frequentist</a>
interpretations of probability, which think of probabilities as a measure of how
often an event would occur if we repeated the circumstances.</p>
</li>
<li>
<p>Subjectivists argue that the likelihood in question is a subjective, epistemic
feature of world. An example of this are
<i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Bayesian_probability"
  target="_blank">Bayesian</a> interpretations
of probability, which think of probabilities as a measure of our <em>degree of
belief</em> in an event occurring.</p>
</li>
</ul>
<p>Here, we won&rsquo;t take sides in this debate. We&rsquo;ll work with a pre-theoretic notion
of probability and focus on the formalism.</p>
<p>Probabilities play many important roles in AI research. On the one hand, they
are the basis for the statistical methods used in modern subsymbolic <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Machine_learning"
  target="_blank">machine
learning</a> methods.  On the other
hand, probabilities are the main method for dealing with inductive inference and
uncertainty in symbolic settings, such as expert systems.</p>
<p>Different applications of probabilities in AI use different (though equivalent)
ways of developing the formalism. Here, we&rsquo;ll develop a <strong>symbolic approach to
probabilities</strong>, which models probabilities as real numbers assigned to formulas
in formal languages.</p>
<p>But you should be aware of the fact that <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Probability_theory"
  target="_blank">mathematical probability
theory</a>, in general, is a
broader theory that can be applied in many different, symbolic and subsymbolic
settings.</p>
<p>For now, we work (


<a href="https://logicalmethods.ai/textbook/many-valued//#syntax"
   
   
   
   
   
   >again</a>

) with a propositional language <span class="excalifont">\mathcal{L}</span> with variables
<span class="excalifont">p_1,\dots,p_n</span> and connectives <span class="excalifont">\neg,\land,\lor</span>.</p>
<p>A <strong>probability</strong> for <span class="excalifont">\mathcal{L}</span> is a function <span class="excalifont">Pr:\mathcal{L}\to\mathbb{R}</span>, which assigns <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Real_number"
  target="_blank">real numbers</a> to formulas, subject to the following three conditions:</p>
<ol>
<li>
<p><span class="excalifont">Pr(A)\geq 0</span>, for all <span class="excalifont">A\in\mathcal{L}</span></p>
</li>
<li>
<p><span class="excalifont">Pr(A)=1</span>, whenever <span class="excalifont">\vDash A</span> (i.e. <span class="excalifont">A</span> is a classical logical truth)</p>
</li>
<li>
<p><span class="excalifont">Pr(A\lor B)=Pr(A)+Pr(B)</span>, whenever <span class="excalifont">\vDash \neg (A\land B)</span> (i.e. <span class="excalifont">A\land B</span> is unsatisfiable)</p>
</li>
</ol>
<p>These are the so-called <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Probability_axioms"
  target="_blank">Kolmogorov axioms</a> for classical probability theory. Note that they use notions from classical logic (logical truth and satisfiability).<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>These are only the basic axioms, there are also <strong>derived laws</strong>. For example,
it&rsquo;s easy to show that for all formulas <span class="excalifont">A</span>, we have: <div class="text-center my-4 excalifont">P(\neg A)=1-P(A).</div></p>
<p>To see this, we can reason as follows:</p>
<ul>
<li>We know that <span class="excalifont">\vDash \neg(A\land \neg A)</span> using Boolean logic.</li>
<li>It follows that <span class="excalifont">Pr(A\lor \neg A)=Pr(A)+Pr(\neg A)</span> by the third axiom.</li>
<li>We further know that <span class="excalifont">\vDash A\lor \neg A</span> using Boolean logic.</li>
<li>It follows that <span class="excalifont">Pr(A\lor \neg A)=1</span> from the second axiom.</li>
<li>Putting it all together, it follows <span class="excalifont">1=Pr(A)+Pr(\neg A)</span>, which gives our
claim by simple algebra (subtracting <span class="excalifont">Pr(A)</span> on both sides).</li>
</ul>
<p>In a similar way, you can show rules like the following:</p>
<ul>
<li><span class="excalifont">Pr(A)=0</span>, whenever <span class="excalifont">\vDash \neg A</span></li>
<li><span class="excalifont">Pr(A\lor B)=Pr(A)+Pr(B)-Pr(A\land B)</span> (the &ldquo;inclusion-exclusion&rdquo; principle)</li>
<li>If <span class="excalifont">A\vDash B</span>, then <span class="excalifont">Pr(A)\leq Pr(B)</span>.</li>
<li>If <span class="excalifont">A</span> and <span class="excalifont">B</span> are equivalent, then <span class="excalifont">Pr(A)=Pr(B)</span>.</li>
</ul>
<p>These kinds of laws hold for <em>all</em> probabilities. And importantly, there are
many possible probability measures, which correspond to different modeling
situations. But how do we give a concrete probability? Well, we have to give a
rule that specifies the probability of every formula. Let&rsquo;s look at an example.</p>
<p>Suppose we&rsquo;re modeling the throw of a single, 6-sided die. Correspondingly, we
have six propositional variables in our language:</p>
<div class="text-center my-4 excalifont">\mathsf{RESULT}_1,\mathsf{RESULT}_2,\mathsf{RESULT}_3,\mathsf{RESULT}_4,\mathsf{RESULT}_5,\mathsf{RESULT}_6,</div>
where <span class="excalifont">\mathsf{RESULT}_i</span> means that the result of the die-roll is <span class="excalifont">i</span>.
<p>If we&rsquo;re dealing with a fair die, we can define a corresponding probability
measure <span class="excalifont">Pr_{\text{fair}}</span> as follows:</p>
<div class="text-center my-4 excalifont">Pr_{\text{fair}}(\mathsf{RESULT}_i)=\frac{1}{6}\text{, for }i=1,\dots,6</div>
<div class="text-center my-4 excalifont">Pr\_{\text{fair}}(\mathsf{RESULT}_i\land \mathsf{RESULT}_j)=0\text{, for all }i\neq
j</div>
<div class="text-center my-4 excalifont">Pr\_{\text{fair}}(\neg\mathsf{RESULT}_1\land \dots\neg\mathsf{RESULT}_6)=0</div>
<p>The third rule ensures that we have at least one result: it&rsquo;s impossible that
the result isn&rsquo;t <span class="excalifont">1</span>, isn&rsquo;t <span class="excalifont">2</span>, <span class="excalifont">\dots</span>, isn&rsquo;t <span class="excalifont">6</span>.</p>
<p>The second rule formalizes the fact that a die can&rsquo;t show two numbers at the
same time, it&rsquo;s impossible that the result is both a <span class="excalifont">1</span> and a <span class="excalifont">6</span>, say. In
other words, we assume that the results are mutually exclusive.</p>
<p>The first rule, then, captures the fact that each result is equally likely, i.e.
the die is fair. Since there are <span class="excalifont">6</span> mutually exclusive results, the only way to
achieve this is by assigning <span class="excalifont">\frac{1}{6}</span> to each.</p>
<p>It turns out that these two rules are sufficient to define a value
<span class="excalifont">Pr_\text{fair}(A)</span> for each formula <span class="excalifont">A</span>. Showing this is not hard, but not
trivial either.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> Here we won&rsquo;t go into the details, but instead
highlight that <strong>probabilities are not recursive</strong>.</p>
<p>That is, the first rule is <em>not</em> sufficient to calculate the probabilities of all
formulas. To calculate the probability of the die being 2 or 4, for example,</p>
<div class="text-center my-4 excalifont">Pr_\text{fair}(\mathsf{RESULT}_2\lor \mathsf{RESULT}_4),</div>
<p>we need both rules:</p>
<ul>
<li>We know from the derived inclusion-exclusion principle that: <div class="text-center my-4 excalifont">Pr_\text{fair}(\mathsf{RESULT}_2\lor \mathsf{RESULT}_4)=</div></li>
</ul>
<div class="text-center my-4 excalifont">Pr_\text{fair}(\mathsf{RESULT}\_2)+Pr\_\text{fair}(\mathsf{RESULT}\_4)-Pr(\mathsf{RESULT}\_2\land \mathsf{RESULT}\_4)</div>
+ We further know from the second rule that: <div class="text-center my-4 excalifont">Pr(\mathsf{RESULT}\_2\land \mathsf{RESULT}\_4) = 0.</div>
+ So we get that: <div class="text-center my-4 excalifont">Pr_\text{fair}(\mathsf{RESULT}\_2\lor \mathsf{RESULT}\_4)=Pr_\text{fair}(\mathsf{RESULT}\_2)+Pr\_\text{fair}(\mathsf{RESULT}\_4).</div>
+ Since by the first rule, we have <span class="excalifont">Pr_\text{fair}(\mathsf{RESULT}\_2)=\frac{1}{6}</span> and <span class="excalifont">Pr_\text{fair}(\mathsf{RESULT}\_4)=\frac{1}{6}</span>, we know that:
<div class="text-center my-4 excalifont">Pr_\text{fair}(\mathsf{RESULT}\_2\lor \mathsf{RESULT}\_4)=\frac{1}{6}+\frac{1}{6}=\frac{1}{3}</div>
<p>But note that crucially, we needed <strong>both rules</strong> in this calculation. The
probabilities of the propositional variables are not enough, we also need the
values of their conjunctions.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<p>In our simple example, this was easy: no conjunction could be true. But in more
complex modeling scenarios, that makes specifying probabilities explicitly
fairly difficult. This is why we&rsquo;ll now describe a method for defining
probabilities that&rsquo;s easier to implement.</p>
<h2 id="probability-truth-tables">Probability truth-tables<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h2>
<hr>

<p>It turns out that we can use <strong>truth-tables</strong> to specify probabilities for a whole
language.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> To see how this works, let&rsquo;s discuss the abstract theory
first, and then go through a concrete example.</p>
<p>The basic idea is that we can specify probabilities by saying how likely
different <em>valuations</em> are. Here, we think of valuations again as logically
possible scenarios and of the likelihood of a valuation as how likely it is that
the things in the actual world turn out exactly like in the scenario.</p>
<p>So, we have <span class="excalifont">\mathsf{RAIN}</span> and <span class="excalifont">\mathsf{SUN}</span> as our propositional variables,
then we have 4 different valuations:</p>
<ul>
<li><span class="excalifont">\nu_1(\mathsf{RAIN})=1</span> and <span class="excalifont">\nu_1(\mathsf{SUN})=1</span></li>
<li><span class="excalifont">\nu_2(\mathsf{RAIN})=1</span> and <span class="excalifont">\nu_2(\mathsf{SUN})=0</span></li>
<li><span class="excalifont">\nu_3(\mathsf{RAIN})=0</span> and <span class="excalifont">\nu_3(\mathsf{SUN})=1</span></li>
<li><span class="excalifont">\nu_4(\mathsf{RAIN})=0</span> and <span class="excalifont">\nu_4(\mathsf{SUN})=0</span></li>
</ul>
<p>These correspond to the &ldquo;world&rdquo; where it rains and the sun shines (<span class="excalifont">\nu_1</span>), it
rains but the sun doesn&rsquo;t shine (<span class="excalifont">\nu_2</span>), and so forth.</p>
<p>We now measure with a <strong>probability weight</strong> <span class="excalifont">m</span> how likely each of these
scenarios is. For example, we could say:</p>
<ul>
<li><span class="excalifont">m(\nu_1)=0.5</span></li>
<li><span class="excalifont">m(\nu_2)=0.3</span></li>
<li><span class="excalifont">m(\nu_3)=0.2</span></li>
<li><span class="excalifont">m(\nu_4)=0</span></li>
</ul>
<p>What this means is that the likelihood of it raining and the sun shining is 0.5,
the likelihood of it raining and the sun not shining 0.3, and so forth.</p>
<p>We could use any weight distribution here, as long as they sum up to <span class="excalifont">1</span>. This
expresses the assumption that at least one of the different scenarios <em>will</em>
happen.</p>
<p>So, more generally, a <strong>probability weight</strong> for our language is a function <span class="excalifont">m</span>,
which assigns to each valuation <span class="excalifont">\nu</span> a positive value <span class="excalifont">0\leq m(\nu)</span> such that
these values sum up to one: <div class="text-center my-4 excalifont">\sum_{\nu}m(\nu)=1</div></p>
<p>Once we have these values, we can calculate the probability of each formula by adding
the probability weights of the worlds, where the formula is true:</p>
<div class="text-center my-4 excalifont">Pr_m(A)=\sum_{\nu(A)=1}m(\nu).</div>
<p>So, in our example, to get the probability of <span class="excalifont">\mathsf{RAIN},</span>
for example, we&rsquo;d calculate</p>
<div class="text-center my-4 excalifont">Pr(\mathsf{RAIN})=\sum_{\nu(\mathsf{RAIN})=1}
m(\nu)=</div>
<div class="text-center my-4 excalifont">m(\nu_1)+m(\nu_2)=0.5+0.3=0.8</div>
<p>That is, in our model, the probability of it raining is 0.8. This is because the
only scenarios where it rains are <span class="excalifont">\nu_1</span> and <span class="excalifont">\nu_2</span> and they have a combined
likelihood of 0.8.</p>
<p>It turns out, as a matter of mathematical fact, that <em>every</em> probability
function <span class="excalifont">Pr</span> can be expressed in this way. We shall not look into the proof of
this, but rather how to use this fact as a convenient method for specifying
probabilities.</p>
<p>The point is that it&rsquo;s much easier to specify a weight for each valuation than
to assign a value to each formula: there are typically way fewer valuations than
relevantly distinct formulas. Which brings us to the connection with
truth-tables.</p>
<p>Remember from 


<a href="https://logicalmethods.ai/textbook/sat//#truth-tables"
   
   
   
   
   
   > Chapter 5.2
</a>

 that in a truth-table, each line corresponds to a
valuation:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/prob-table-basis.png" class="img-thumbnail mx-auto d-block my-4"    alt="" >

<p>This means that in the table, we can conveniently give the information of our
measure as follows:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/prob-table-weight.png" class="img-thumbnail mx-auto d-block my-4"    alt="" >

<p>So far, so good. What makes this method really powerful is that it allows us to
calculate the probability of a formula: simply calculate the values of the
formula for each row in the usual way and then sum up the <span class="excalifont">m</span>-values for each
row where the formula gets value <span class="excalifont">1</span>.</p>
<p>So, in our example, if we want to know the probability of
<span class="excalifont">\mathsf{RAIN}\lor\neg\mathsf{SUN}</span>, we&rsquo;d proceed as follows:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/prob-table-full.png" class="img-thumbnail mx-auto d-block my-4"    alt="" >

<p>Then, we can see that:</p>
<div class="text-center my-4 excalifont">Pr(\mathsf{RAIN}\lor\neg\mathsf{SUN})=m(\nu_1)+m(\nu_2)+m(\nu_4)=0.5+0.3+0=0.8</div>
<p>This is a powerful method for representing and calculating probabilities. But it
also has its limits. For example, it suffers from the same problem of
<strong>combinatorial explosion</strong> as the truth-tables method for validity checking.
For example, writing the full truth-table for our die example from above, we
need <span class="excalifont">2^6=64</span> rows. That means that we need to use &ldquo;tricks&rdquo; to handle them
efficiently. In this example, we can use the fact that every row where more than
one <span class="excalifont">\mathsf{RESULT}_i</span> gets value <span class="excalifont">1</span> will have <span class="excalifont">m</span>-value <span class="excalifont">0</span>, so we can leave
it out of the table:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/prob-table-die.png" class="img-thumbnail mx-auto d-block my-4"    alt="" >

<p>Unfortunately, computationally efficient implementations of probabilities are
beyond what we can discuss in this setting.</p>
<h2 id="conditional-probabilities">Conditional probabilities<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h2>
<hr>

<p>Having outlined what probabilities are and how we can describe them, we now turn
our attention to modeling inductive inference using probabilities. As we&rsquo;ve
discussed already in 


<a href="https://logicalmethods.ai/textbook/valid-inference//#semantic-methods-for-induction"
   
   
   
   
   
   > Chapter 2. Valid inference</a>

, the idea is to use <strong>conditional probabilities</strong> for this
purpose.</p>
<p>The conditional probability of one formula <span class="excalifont">A</span> <em>given</em> another formula <span class="excalifont">B</span> is,
intuitively, a measure of how likely it is that <span class="excalifont">A</span> is true assuming that  <span class="excalifont">B</span>
is true. This idea is formally captured in the definition:</p>
<div class="text-center my-4 excalifont">Pr(A\mid B)=\frac{Pr(A\land B)}{Pr(B)}</div>
<p>Here we need to assume that <span class="excalifont">Pr(B)\neq 0</span> to avoid <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Division_by_zero"
  target="_blank">division by
0</a>.</p>
<p>Let&rsquo;s look at how this works in some examples:</p>
<ul>
<li>In the <span class="excalifont">\mathsf{RAIN},\mathsf{SUN}</span> example, we could ask, for example, what
the probability is that it rains given that the sun is shining:</li>
</ul>
<div class="text-center my-4 excalifont">\mathsf{Pr}(\mathsf{RAIN}\mid\mathsf{SUN})=\frac{Pr(\mathsf{SUN}\land\mathsf{RAIN})}{Pr(\mathsf{SUN})}</div> 
<p>For this calculation, we need to know <span class="excalifont">Pr(\mathsf{RAIN}\land\mathsf{SUN}),Pr(\mathsf{SUN})</span>. We get them from the following table:</p>

  <img src="https://logicalmethods.ai/textbook/probability/img/prob-table-rain.png" class="img-thumbnail mx-auto d-block my-4"    alt="" >

<p>We get <span class="excalifont">Pr(\mathsf{RAIN}\land\mathsf{SUN})=0.5</span> and
<span class="excalifont">Pr(\mathsf{SUN})=0.5+0.2=0.7</span>. We get:</p>
  <div class="text-center my-4 excalifont">\mathsf{Pr}(\mathsf{RAIN}\mid\mathsf{SUN})=\frac{0.5}{0.7}\approx 0.71</div>
<p>So, the answer is that the chance of it raining given that the sun is shining
is around 71%.</p>
<ul>
<li>
<p>Let&rsquo;s do another example to drive the point home. In our die example, let&rsquo;s
ask how likely it is to roll a 2 given that we roll an even number. Abbreviating
<span class="excalifont">\mathsf{RESULT}_i</span> to <span class="excalifont">\mathsf{R}_i</span>, we can represent  rolling an
even number with the formula <div class="text-center my-4 excalifont">\mathsf{R}_2\lor\mathsf{R}_4\lor\mathsf{R}_6.</div></p>
<p>Our question this is:</p>
<div class="text-center my-4 excalifont">Pr(\mathsf{R}_2\mid \mathsf{R}_2\lor\mathsf{R}_4\lor\mathsf{R}_6 )=\frac{Pr(\mathsf{R}_2\land (\mathsf{R}_2\lor\mathsf{R}_4\lor\mathsf{R}_6))}{Pr(\mathsf{R}_2\lor\mathsf{R}_4\lor\mathsf{R}_6)}</div>
<p>We use the following simplified truth-table for the calculation:</p>

  <img src="https://logicalmethods.ai/textbook/probability/img/prob-table-even.png" class="img-thumbnail mx-auto d-block my-4"    alt="" >

<p>We get:</p>
<div class="text-center my-4 excalifont">Pr(\mathsf{R}_2\lor\mathsf{R}_4\lor\mathsf{R}_6)=\frac{1}{6}+\frac{1}{6}+\frac{1}{6}=\frac{1}{2}</div>
<div class="text-center my-4 excalifont">Pr(\mathsf{R}_2\land (\mathsf{R}_2\lor\mathsf{R}_4\lor\mathsf{R}_6))=\frac{1}{6}</div>
<p>So:</p>
</li>
</ul>
 <div class="text-center my-4 excalifont">Pr(\mathsf{R}_2\mid \mathsf{R}_2\lor\mathsf{R}_4\lor\mathsf{R}_6 )=\frac{\frac{1}{6}}{\frac{1}{2}}=\frac{1}{3}</div>
  That is, the chance of getting a 2 _given_ that you get an even number is
  <span class="excalifont">\frac{1}{3}</span>.
<p>Besides helping us to define valid inductive inference, which we&rsquo;ll turn to in
the next section, conditional probabilities have a lot of theoretical use.</p>
<p>One important application that underlies many practical applications is to
define the notion of <strong>probabilistic independence</strong>. We say that two formulas
<span class="excalifont">A,B</span> are probabilistically independent iff <div class="text-center my-4 excalifont">Pr(A|B)=Pr(A).</div></p>
<p>If <span class="excalifont">A,B</span> are probabilistically independent, then whether <span class="excalifont">B</span> occurs &ldquo;doesn&rsquo;t
matter&rdquo; for whether <span class="excalifont">A</span> occurs.</p>
<p>For example, in the following example, we have that whether it rains and the sun
is shining are independent:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/prob-table-indep.png" class="img-thumbnail mx-auto d-block my-4"    alt="" >

<p>To see this, consider:</p>
<div class="text-center my-4 excalifont">Pr(\mathsf{SUN}\mid
\mathsf{RAIN})=\frac{Pr(\mathsf{SUN}\land\mathsf{RAIN})}{Pr(\mathsf{RAIN})}</div>
<div class="text-center my-4 excalifont">=\frac{0.5}{0.5+0.3=0.8}=0.625=0.5+0.125=Pr(\mathsf{SUN})</div>
<p>It&rsquo;s worth noting that if <span class="excalifont">A,B</span> are probabilistically independent, we can
calculate the probability of <span class="excalifont">A\land B</span> from the probabilities of <span class="excalifont">A,B</span>:</p>
<ul>
<li>If <span class="excalifont">Pr(A|B)=Pr(A)</span>, then <span class="excalifont">Pr(A\land B)=Pr(A)\times Pr(B)</span>.</li>
</ul>
<p>This holds in our example, since</p>
<div class="text-center my-4 excalifont">Pr(\mathsf{RAIN})=0.5+0.3=0.8</div>
<div class="text-center my-4 excalifont">Pr(\mathsf{SUN})=0.5+0.125=0.625</div>
<div class="text-center my-4 excalifont">Pr(\mathsf{RAIN}\land\mathsf{SUN})=0.5=0.8\times 0.625</div>
<p>But it&rsquo;s important to note that this <em>doesn&rsquo;t</em> hold in general. In the original
table:</p>

<img src="https://logicalmethods.ai/textbook/probability/img/prob-table-rain.png" class="img-thumbnail mx-auto d-block my-4"    alt="" >

<p>We have:</p>
<div class="text-center my-4 excalifont">Pr(\mathsf{RAIN})=0.5+0.3=0.8</div>
<div class="text-center my-4 excalifont">Pr(\mathsf{SUN})=0.5+0.2=0.7</div>
<div class="text-center my-4 excalifont">Pr(\mathsf{RAIN}\land\mathsf{SUN})=0.5\neq 0.8\times 0.7=0.56</div>
<h2 id="inductive-validity">Inductive validity<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h2>
<hr>

<p>It is now time to turn to <strong>inductive validity</strong>. But before we give an
account, it is important to note that inductive logic is a <em>broad field</em>, which
is closely related to <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Statistical_inference"
  target="_blank">statistical
inference</a>. Given this, we
should note that the account of inductive validity we present here is just
<em>one</em> view of valid inference in a narrow sense.</p>
<p>The situation is similar to the one we face in deductive logic: Boolean logic,
many-valued logic, fuzzy logic, etc. all give slightly different models of
deductively valid inference, which are applicable in different circumstances,
under different assumptions, etc.</p>
<p>Correspondingly, there are different models of inductive inference. The model
we&rsquo;ll discuss in this section is a simple version of what&rsquo;s known as <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Bayesian_inference"
  target="_blank">Bayesian
inference</a>. But we won&rsquo;t go
into the details of <strong>Bayesian statistics</strong> and the difference to other paradigms.
Instead, we&rsquo;ll focus on the idea of using conditional probability as a measure
of inductive support, which is especially important in Bayesian statistics but
on a general level, is characteristic of inductive inference.</p>
<p>The basic idea we&rsquo;re pursuing is, as discussed in 


<a href="https://logicalmethods.ai/textbook/valid-inference/"
   
   
   
   
   
   > Chapter 2. Valid inference </a>

,
the idea that an inference is inductively valid just in case the premises make
the conclusion more likely. We interpret this &ldquo;more likely&rdquo; as: the conditional
probability of the conclusion given the premises is higher than the
unconditional probability of the conclusion.</p>
<p>Using the symbol <span class="excalifont">\mid\approx</span> for inductive validity, the definition is:</p>
<div class="text-center my-4 excalifont"> P_1,P_2,\dots\mid\approx C \Leftrightarrow Pr(C|P_1\land P_2\land
\dots)\geq Pr(C)</div>
This concept is essentially what [Rudolf
Carnap](https://en.wikipedia.org/wiki/Rudolf_Carnap) calls the **increase of
firmness** concept of evidential support: the premises, if we were to add them
to our knowledge bank, would raise the degree with which our KB supports the
conclusion.
<p>There are a few things to note about the definition, at this point:</p>
<ul>
<li>
<p>The right-hand side of this definition is only well-defined if <span class="excalifont">Pr(P_1\land
P_2\land \dots)\neq 0</span>. That is, we need to assume that the premises are
probabilistically consistent. What should we say if they are not? The natural
answer would be that the inference should be inductively valid in the same way
an inference with classically inconsistent premises is deductively valid.</p>
</li>
<li>
<p>The concept of inductive validity depends on a background probability. That
is, technically speaking, our concept of <span class="excalifont">\mid\approx</span> is <em>relative</em> to <span class="excalifont">Pr</span> and
should be written <span class="excalifont">\mid\approx_{Pr}</span>. In this sense, our notion of what
constitutes a valid inductive inference depends on what we already know (or
believe) about the world. This is an issue we&rsquo;ll be revising in the next
chapter, when we&rsquo;ll explore logic-based learning. In practice, however, we often
leave out the subscript <span class="excalifont">Pr</span> and simply write <span class="excalifont">\mid\approx</span>.</p>
</li>
<li>
<p>There are, however, <strong>laws of inductive logic</strong> which don&rsquo;t depend on the
choice of <span class="excalifont">Pr</span> and are therefore purely logical. We&rsquo;ll discuss some
examples below.</p>
</li>
<li>
<p>The notion of inductive support given by the above definition is <em>weak</em>. <em>Any</em>
increase of firmness for the conclusion constitutes an inductively valid
inference, not matter how small. This means that having an inductively valid
inference for the conclusion from true premises is by itself not a reason to
believe the conclusion.</p>
<p>For this, we need <strong>inductively strong</strong> inferences. These are inferences,
where the conditional probability of the conclusion given the premises is
<em>high</em>. The simplest version of this idea is where we say that:</p>
<div class="text-center my-4 excalifont"> P_1,P_2,\dots\mid\overset{!}{\approx} C \Leftrightarrow Pr(C|P_1\land P_2\land
\dots)\gg Pr(C),</div>
<p>where <span class="excalifont">x\gg y</span> means that <span class="excalifont">x</span> is &ldquo;much&rdquo; bigger than <span class="excalifont">y</span>. What &ldquo;much bigger&rdquo;
means can then depend on the context, such as, for example, what <span class="excalifont">Pr(C)</span> is.
An inductively strong inference may push <span class="excalifont">Pr(C)</span> from <span class="excalifont">0.9</span> to <span class="excalifont">0.99</span> or from
<span class="excalifont">0.5</span> to <span class="excalifont">0.9</span>. What counts as a</p>
<p>But there are alternative notions as well: we could, for example, set a
<strong>threshold</strong> <span class="excalifont">\epsilon</span>, typically bigger than <span class="excalifont">0.5</span>, for how high the
probability of a conclusion needs to become in order for the inference to
count as strong:</p>
<div class="text-center my-4 excalifont"> P_1,P_2,\dots\mid\overset{!}{\approx} C \Leftrightarrow Pr(C|P_1\land P_2\land
\dots)\geq \epsilon>0.5.</div> A good inductive inference is one which makes the
conclusion more likely than not.
<p>The upshot is that in inductive logic, there is not as much uniformity as in
deductive logic: different notions are available and there&rsquo;s absolute
agreement on what the &ldquo;standard&rdquo; concept should be.</p>
</li>
</ul>
<p>We&rsquo;ll continue thinking about the simple increase of firmness concept of
inductive validity <span class="excalifont">\mid\approx</span>. We can note the following <strong>inductive laws</strong>,
which are independent of the choice of <span class="excalifont">Pr</span>:</p>
<ul>
<li>
<p>If <span class="excalifont">P_1,P_2,\dots\vDash C</span>, then <span class="excalifont">P_1,P_2,\dots\mid\approx C</span>.</p>
<p>This follows from the above observation that if <span class="excalifont">A,B</span> are equivalent, then
<span class="excalifont">Pr(A)=Pr(B)</span>. Because if <span class="excalifont">P_1,P_2,\dots\vDash C</span>, then, as a matter of
mathematical fact we won&rsquo;t show in detail, <span class="excalifont">P_1\land P_2\land\dots \land C</span> is
equivalent to <span class="excalifont">P_1\land P_2\land\dots</span>. This gives us: <div class="text-center my-4 excalifont">Pr(C\mid
P_1,P_2,\dots)=\frac{Pr(P_1\land P_2\land \dots\land C)}{Pr(P_1\land P_2\land
\dots)}=\frac{Pr(P_1\land P_2\land \dots)}{Pr(P_1\land P_2\land \dots)}=1</div></p>
<p>Since <span class="excalifont">1\geq Pr(C)</span> for each <span class="excalifont">C</span>, this proves our claim.</p>
</li>
<li>
<p>If <span class="excalifont">C\vDash P_1\land P_2\land \dots</span>, then <span class="excalifont">P_1\land P_2\land \dots\mid\approx
C</span>.</p>
<p>This follows from the fact that if <span class="excalifont">A\vDash B</span>, then <span class="excalifont">Pr(A)\leq Pr(B)</span>, we
observed above. Because from this it follows that if <span class="excalifont">C\vDash P_1\land P_2\land \dots</span>, then <span class="excalifont">Pr(C)\leq Pr(P_1\land P_2\land \dots)</span>. Further, as before we have that <span class="excalifont">C\land P_1\land P_2\land \dots</span> is equivalent to <span class="excalifont">C</span> and so, we can reason as follows:</p>
<div class="text-center my-4 excalifont">Pr(C\mid P_1\land P_2\land \dots)=\frac{Pr(C\land P_1\land
P_2\land\dots)}{Pr(P_1\land P_2\land\dots)}</div>
<div class="text-center my-4 excalifont">=\frac{Pr(C)}{Pr(P_1\land P_2\land\dots)}\overset{Pr(C)\leq Pr(P_1\land P_2\land \dots)}\geq Pr(C)</div>
</li>
</ul>
<p>The latter law will allow us to observe the inductive validity of some
well-known principles, one of which we&rsquo;ll discuss on the following section.</p>
<p>The law is a sort of mathematical proof of the so-called <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Deductive-nomological_model"
  target="_blank">deductive-nomological
model</a> of
confirmation, which though not unproblematic has been hugely influential in
thinking about what confirmation is.</p>
<h2 id="validities-and-fallacies">Validities and fallacies<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h2>
<hr>

<p>To conclude our theoretical discussion of inductive inference, let&rsquo;s discuss two
concrete examples of inductive inference, one valid, one invalid.</p>
<h3 id="enumerative-induction">Enumerative induction<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h3>
<hr>

<p><strong>Enumerative induction</strong> is the principle that we can infer <span class="excalifont">\forall x P(x)</span>
from sufficiently many and well-chosen instances <span class="excalifont">P(a_1), P(a_2),\dots</span>. In our
simple setting, we can express this as: <div class="text-center my-4 excalifont">P(a_1),P(a_2),\dots\mid\approx \forall
xP(x).</div> The simple, standard example of this inference is that all observed
swans, sampled well and wide, were white, so all swans are white. Of course, this
inference is deductively invalid—it can always happen that there&rsquo;s a non-white
swan somewhere—but we can show, mathematically, that the inference is
inductively valid in the (weak) increase of firmness sense.</p>
<p>This follows from the previous law that if the conclusion implies the premises,
an inference is inductively valid. Let&rsquo;s suppose for a moment that we have
extended our treatment of probabilities to a FOL language, which is rather
straight-forward but takes a lot of space.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> Assuming that we still get our
theorem (which we do), we can infer the validity of enumerative induction from
the simple fact that <div class="text-center my-4 excalifont">\forall xP(x)\vDash P(a_i),</div> for each <span class="excalifont">i</span> following the
law of universal instantiation.</p>
<p>Even more so, given some reasonable assumptions, since the function</p>
<div class="text-center my-4 excalifont">Pr(\forall xP(x)\mid \dots)</div> grows monotonically, if we put every increasing
numbers of instances in[^math]
<div class="text-center my-4 excalifont">P(a_1)</div> 
<div class="text-center my-4 excalifont">P(a_1)\land P(a_2)</div> 
<div class="text-center my-4 excalifont">P(a_1)\land P(a_2)\land \dots</div> 
we will eventually "crack" every threshold of <span class="excalifont">\epsilon\geq 0.5</span> we could chose.
<p>This means that on our simple model of inductive validity, for sufficiently
large <span class="excalifont">n</span>, <div class="text-center my-4 excalifont">P(a_1),\dots,P(a_n)\mid\overset{!}{\approx}\forall xP(x)</div></p>
<h2 id="conjunction-fallacy">Conjunction fallacy<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h2>
<hr>

<p>Here&rsquo;s the original <strong>Linda problem</strong> from Tversky and Kahneman:</p>
<p><em>Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.</em></p>
<p><em>Which is more probable?</em></p>
<ol>
<li><em>Linda is a bank teller.</em></li>
<li><em>Linda is a bank teller and is active in the feminist movement.</em></li>
</ol>
<p>In experimental surveys, people reliably judge option 2 more likely as option 1,
which is a <strong>probabilistic fallacy</strong>.</p>
<p>We can now see why: Since <div class="text-center my-4 excalifont">\mathsf{BANK}\land\mathsf{FEMINIST}\vDash
\mathsf{BANK},</div> we can infer that <div class="text-center my-4 excalifont">Pr(\mathsf{BANK}\land\mathsf{FEMINIST})\leq
Pr(\mathsf{BANK}),</div> for any <span class="excalifont">Pr</span> we could chose.</p>
<p>There is an extensive psychological literature that tries to explain <em>why</em>
people make this and related fallacies. But for us, from a logical perspective,
the crucial point is <em>that</em> people do. In symbolic inductive reasoning, we have
to be careful not fall into the trap of these fallacies, and the best way is to
use logical methods to model inductive reasoning.</p>
<h2 id="further-readings">Further readings<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h2>
<hr>

<p>An excellent introduction to the formalism of Bayesian epistemology is:</p>
<ul>
<li><i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://global.oup.com/ukhe/product/fundamentals-of-bayesian-epistemology-1-9780198707615?cc=gb&amp;lang=en&amp;"
  target="_blank">Titelbaum, Michael. 2022. Fundamentals of Bayesian Epsitemology I. Oxford
University Press</a>.</li>
</ul>
<p>Chapters 12 and 13 of <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://elibrary.pearson.de/book/99.150005/9781292401171"
  target="_blank">Russel and Norvig</a> are an excellent introduction to more detailed uses of probabilistic reasoning in AI.</p>
<p><strong>Notes</strong>:</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>There are also <em>non</em>-classical probability theories, which build on alternative background logics. For example, there is strong Kleene probability theory. Check out <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://eprints.whiterose.ac.uk/104891/1/nonclassicalprobabilityfinal.pdf"
  target="_blank">this paper</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>The proof involves a different normal form than the CNFs we used in



<a href="https://logicalmethods.ai/textbook/sat//#normal-forms"
   
   
   
   
   
   > Chapter 5. Boolean
satisfiability </a>

, called <i class="bi bi-box-arrow-up-right h-6"></i>
<a class="link-dark" href="https://en.wikipedia.org/wiki/Disjunctive_normal_form"
  target="_blank">disjunctive normal forms
(DNFs)</a>. It also uses the
facts that equivalent formulas have the same probability and that if one formula
implies the other the probability of the one is lower than that of the other.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>&hellip; for example. There are other ways of specifying a probability, using
all probabilities of disjunctions, for example. But that&rsquo;s not the important
point.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>The only constraint is that we only have finitely many propositional
variables <span class="excalifont">p_1,\dots,p_n</span> and not infinitely many <span class="excalifont">p_1,p_2,\dots</span>. But in
concrete, AI modeling situations, this isn&rsquo;t usually a practical problem.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>See the Titelbaum book in the suggested readings for how to do this
precisely.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>
   <p><em>Last edited: </em>21/10/2024</p>
</div>


<ul class="navbar-nav flex-row justify-content-between mt-auto">
  
  <li class="nav-item">
    <a href="/textbook/many-valued/" class="btn" tabindex="-1" role="button" aria-disabled="true">
      <i class="bi bi-chevron-left"></i>
    </a>
    
  </li>
  <li class="nav-item">
    <button class="btn btn-back-to-top">
    <i class="bi bi-chevron-double-up"></i>
  </button>
  </li>
  <li class="nav-item">
    
    <a href="/textbook/learning/" class="btn" tabindex="-1" role="button" aria-disabled="true">
      <i class="bi bi-chevron-right"></i>
    </a>
    
  </li>
</ul>

    </main>
    <footer class="align-self-center d-flex flex-column mt-md-1 p-0" data-bs-theme="dark">
      <nav class="navbar bg-dark shadow" data-bs-theme="dark" aria-label="footer-navbar">
  <div class="container-sm align-self-center px-3">
    <a class="navbar-brand fs-6" href="/about" aria-label="copyright-link">&copy; 2025 jkorbmacher et al. </a>
    <ul class="navbar-nav">
      <li class="nav-item">
        <a class="nav-link fs-6" href="https://github.com/jkorb/logicalmethods.ai" target="_blank"><i class="bi bi-github"></i></a>
      </li>
    </ul>
  </div>
</nav>
 
      <script src="https://logicalmethods.ai/js/helpers.js"></script>
      <script src="https://logicalmethods.ai/bootstrap/dist/js/bootstrap.bundle.min.js"></script>
      
      
    </footer>
  </body>
</html>
