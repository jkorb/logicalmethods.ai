<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Textbook on logicalmethods.ai</title>
    <link>https://logicalmethods.ai/textbook/</link>
    <description>Recent content in Textbook on logicalmethods.ai</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Jan 0001 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://logicalmethods.ai/textbook/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Logic and AI</title>
      <link>https://logicalmethods.ai/textbook/logic-and-ai/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://logicalmethods.ai/textbook/logic-and-ai/</guid>
      <description>&lt;h1 id=&#34;logic-and-ai&#34;&gt;Logic and AI&lt;button class=&#34;btn btn-back-to-top&#34;&gt;&#xA;    &#xA;  &lt;/button&gt;&#xA;&lt;/h1&gt;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Logic and AI are intimately connected. In this chapter you&amp;rsquo;ll learn how.&lt;/p&gt;&#xA;&lt;p&gt;At the end of the chapter, you&amp;rsquo;ll be able to:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;define logic and artificial intelligence as scientific disciplines,&lt;/li&gt;&#xA;&lt;li&gt;explain the three main ways in which the two disciplines are related, and&lt;/li&gt;&#xA;&lt;li&gt;give some examples of uses of logical methods in AI and distinguish them from&#xA;non-logical methods.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;By the way, meet &lt;span class=&#34;code&#34; style=&#34;margin-right:-4px;vertical-align: text-top;transform-origin: 50%  50%;transform:rotate(180deg);display:inline-block&#34;&gt;IA&lt;/span&gt;&#xA;  (read: &amp;ldquo;for all I&amp;rdquo;)!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Formal languages</title>
      <link>https://logicalmethods.ai/textbook/formal-languages/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://logicalmethods.ai/textbook/formal-languages/</guid>
      <description>&lt;h1 id=&#34;formal-languages&#34;&gt;Formal languages&lt;button class=&#34;btn btn-back-to-top&#34;&gt;&#xA;    &#xA;  &lt;/button&gt;&#xA;&lt;/h1&gt;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&#xA;&lt;img src=&#34;https://logicalmethods.ai/textbook/formal-languages/img/ai_language.png&#34; class=&#34;rounded  float-start inert-img img-fluid m-2&#34;  width=&#34;350px&#34;  alt=&#34;&#34; &gt;&#xA;&#xA;When we try to develop AI systems, we immediately run into an issue: computers,&#xA;which are the basis for any modern AI technology, &amp;ldquo;speak&amp;rdquo; a different language&#xA;than us—the proverbial 1&amp;rsquo;s and 0&amp;rsquo;s.&lt;/p&gt;&#xA;&lt;p&gt;That is, even if we understand what intelligent behavior is and we manage to&#xA;break it down into instructions that a computer can, in principle, follow, we&#xA;still need to express these instructions in an unambiguous language that the&#xA;computer can &amp;ldquo;understand&amp;rdquo; (i.e. execute).&#xA;&#xA;&lt;img src=&#34;https://logicalmethods.ai/textbook/formal-languages/img/ai_understand.png&#34; class=&#34;rounded  float-end inert-img img-fluid m-2&#34;  width=&#34;350px&#34;  alt=&#34;&#34; &gt;&#xA;&#xA;We need to write &lt;em&gt;code&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Valid Inference</title>
      <link>https://logicalmethods.ai/textbook/valid-inference/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://logicalmethods.ai/textbook/valid-inference/</guid>
      <description>&lt;h1 id=&#34;valid-inference&#34;&gt;Valid inference&lt;button class=&#34;btn btn-back-to-top&#34;&gt;&#xA;    &#xA;  &lt;/button&gt;&#xA;&lt;/h1&gt;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Inference is everywhere in AI. We&amp;rsquo;ve seen how deductive inference occurs on the&#xA;level of circuits using Shannon&amp;rsquo;s interpretation, we&amp;rsquo;ve mentioned that &lt;abbr title=&#34;large language models, such as GPT, Claude, Llama, ...&#34;&gt;&lt;a  class=&#34;link-underline-opacity-0 link-body-emphasis&#34;&gt; LLMs&lt;/a&gt;&lt;/abbr&gt;&#xA;use a form of inductive inference to predict pieces of text, and, of course, any&#xA;&lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Artificial_General_Intelligence&#34;&#xA;  target=&#34;_blank&#34;&gt;artificial general intelligence&#xA;(AGI)&lt;/a&gt; would need&#xA;to be able perform valid inferences, like our first example:&lt;/p&gt;&#xA;&#xA;&lt;img src=&#34;https://logicalmethods.ai/textbook/valid-inference/img/socrates_inference.png&#34; class=&#34;rounded mx-auto d-block inert-img img-fluid my-4&#34;  width=&#34;400px&#34;  alt=&#34;&#34; &gt;&#xA;&#xA;&lt;p&gt;So what precisely makes an inference valid? What distinguishes good from bad&#xA;inferences, both in deductive and inductive reasoning?—In this chapter, we&amp;rsquo;ll&#xA;deep dive into the logical theory of valid inference.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Boolean algebra</title>
      <link>https://logicalmethods.ai/textbook/boolean/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://logicalmethods.ai/textbook/boolean/</guid>
      <description>&lt;h1 id=&#34;boolean-algebra&#34;&gt;Boolean algebra&lt;button class=&#34;btn btn-back-to-top&#34;&gt;&#xA;    &#xA;  &lt;/button&gt;&#xA;&lt;/h1&gt;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&#xA;&lt;img src=&#34;https://logicalmethods.ai/textbook/boolean/img/low_level.png&#34; class=&#34;rounded  float-end inert-img img-fluid mx-3&#34;  width=&#34;300px&#34;  alt=&#34;&#34; &gt;&#xA;&#xA;Boolean algebra—the logic of 0&amp;rsquo;s, 1&amp;rsquo;s, not, and, or, …—is the ultimate logical&#xA;foundation of modern computers, and by extension &lt;em&gt;all&lt;/em&gt; AI systems. The study of&#xA;Boolean logic has its roots in the work of &lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/George_Boole&#34;&#xA;  target=&#34;_blank&#34;&gt;George&#xA;Boole&lt;/a&gt; from the mid 19th century.&#xA;But it wasn&amp;rsquo;t until AI-pioneer &lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Claude_Shannon&#34;&#xA;  target=&#34;_blank&#34;&gt;Claude&#xA;Shannon&lt;/a&gt; connected the theory to&#xA;the basic workings of electronic circuits that the importance of Boolean algebra&#xA;for the development of information technologies became apparent. While Shannon&#xA;was investigating &lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Relay&#34;&#xA;  target=&#34;_blank&#34;&gt;relays&lt;/a&gt;—essentially&#xA;electronically operated switches—and not&#xA;&lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Semiconductor&#34;&#xA;  target=&#34;_blank&#34;&gt;semiconductors&lt;/a&gt;, which are the&#xA;actual technology used to implement computers today, the fundamental principles&#xA;are the same. Boolean algebra is the logic of low-level computing, and the way&#xA;that computers add, subtract, multiply, etc. are ultimately grounded in Boolean&#xA;logic.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Boolean satisfiability</title>
      <link>https://logicalmethods.ai/textbook/sat/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://logicalmethods.ai/textbook/sat/</guid>
      <description>&lt;h1 id=&#34;boolean-satisfiability&#34;&gt;Boolean satisfiability&lt;button class=&#34;btn btn-back-to-top&#34;&gt;&#xA;    &#xA;  &lt;/button&gt;&#xA;&lt;/h1&gt;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;We&amp;rsquo;ve seen that Boolean logic is at the heart of computation and deductive&#xA;reasoning. But when it comes to automated reasoning, our approach has so-far&#xA;been limited to pre-programmed circuits, like the &lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Logic_gate&#34;&#xA;  target=&#34;_blank&#34;&gt;logic&#xA;gates&lt;/a&gt; and&#xA;&lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Adder_%28electronics%29&#34;&#xA;  target=&#34;_blank&#34;&gt;adders&lt;/a&gt; we&amp;rsquo;ve discussed.&#xA;While these circuits are great at carrying out specific deductive reasoning&#xA;tasks, like adding two numbers or calculating truth-values, they are not very&#xA;flexible. Each circuit carries out one specific task.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Logical conditionals</title>
      <link>https://logicalmethods.ai/textbook/conditionals/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://logicalmethods.ai/textbook/conditionals/</guid>
      <description>&lt;h1 id=&#34;logical-conditionals&#34;&gt;Logical conditionals&lt;button class=&#34;btn btn-back-to-top&#34;&gt;&#xA;    &#xA;  &lt;/button&gt;&#xA;&lt;/h1&gt;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&#xA;&lt;img src=&#34;https://logicalmethods.ai/textbook/conditionals/img/ai_wondering.png&#34; class=&#34;rounded  float-start inert-img img-fluid m-2&#34;  width=&#34;200px&#34;  alt=&#34;&#34; &gt;&#xA;&#xA;The &lt;abbr title=&#34;if-then&#34;&gt;&lt;a  class=&#34;link-underline-opacity-0 link-body-emphasis&#34;&gt; conditional&lt;/a&gt;&lt;/abbr&gt; is the backbone of valid&#xA;inference. We&amp;rsquo;ve said that valid inference is based on hypothetical reasoning:&#xA;valid inference requires that the conclusion be true under the hypothesis that&#xA;the premises are—&lt;em&gt;if&lt;/em&gt; the premises are. This applies to both deductive and&#xA;inductive inference, but in this chapter, we&amp;rsquo;ll focus on deductive inference.&lt;/p&gt;&#xA;&lt;p&gt;The conditional of deductive logic, which we formalize as &#xA;&lt;img src=&#34;https://logicalmethods.ai/img/to.png&#34; alt=&#34;Implication&#34; class=&#34;inline-icon&#34; style=&#34;height:1em;width:auto;&#34;/&gt;&#xA;, is the&#xA;basis for many artificial reasoning techniques, especially in logic-based AI.&#xA;It turns out that conditional reasoning is crucial to many AI-related tasks.&#xA;Think for example of computer code like this:&#xA;&#xA;&lt;img src=&#34;https://logicalmethods.ai/textbook/conditionals/img/python-if-then.png&#34; class=&#34;rounded mx-auto d-block inert-img img-fluid&#34;  width=&#34;450px&#34;  alt=&#34;&#34; &gt;&#xA;&#xA;Both to evaluate and understand this code, we need to understand basic&#xA;conditional reasoning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Logical proofs</title>
      <link>https://logicalmethods.ai/textbook/proofs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://logicalmethods.ai/textbook/proofs/</guid>
      <description>&lt;h1 id=&#34;logical-proofs&#34;&gt;Logical proofs&lt;button class=&#34;btn btn-back-to-top&#34;&gt;&#xA;    &#xA;  &lt;/button&gt;&#xA;&lt;/h1&gt;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;A &lt;em&gt;logical proof&lt;/em&gt; is a chaining of inference rules applied to logical formulas,&#xA;which models natural language step-by-step reasoning. We&amp;rsquo;ve already seen a&#xA;simple example of a proof, when we discussed the chaining methods. The following&#xA;structure, for example, is a logical proof that &lt;em&gt;derives&lt;/em&gt; $RAINBOW$ from the&#xA;assumptions $$MORNING, CLEAR, RAIN, MORNING &#xA;&lt;img src=&#34;https://logicalmethods.ai/img/to.png&#34; alt=&#34;Implication&#34; class=&#34;inline-icon&#34; style=&#34;height:1em;width:auto;&#34;/&gt;&#xA; DAY, (CLEAR &#xA;&lt;img src=&#34;https://logicalmethods.ai/img/conjunction.png&#34; alt=&#34;Conjunction&#34; class=&#34;inline-icon&#34; style=&#34;height:1em;width:auto;&#34;/&gt;&#xA;&#xA;DAY)&#xA;&lt;img src=&#34;https://logicalmethods.ai/img/to.png&#34; alt=&#34;Implication&#34; class=&#34;inline-icon&#34; style=&#34;height:1em;width:auto;&#34;/&gt;&#xA; SUN, &amp;hellip;$$ $$&amp;hellip; (MORNING &#xA;&lt;img src=&#34;https://logicalmethods.ai/img/conjunction.png&#34; alt=&#34;Conjunction&#34; class=&#34;inline-icon&#34; style=&#34;height:1em;width:auto;&#34;/&gt;&#xA; SUN)&#xA;&lt;img src=&#34;https://logicalmethods.ai/img/to.png&#34; alt=&#34;Implication&#34; class=&#34;inline-icon&#34; style=&#34;height:1em;width:auto;&#34;/&gt;&#xA; LOW_SUN,&#xA;(RAIN &#xA;&lt;img src=&#34;https://logicalmethods.ai/img/conjunction.png&#34; alt=&#34;Conjunction&#34; class=&#34;inline-icon&#34; style=&#34;height:1em;width:auto;&#34;/&gt;&#xA; LOW_SUN)&#xA;&lt;img src=&#34;https://logicalmethods.ai/img/to.png&#34; alt=&#34;Implication&#34; class=&#34;inline-icon&#34; style=&#34;height:1em;width:auto;&#34;/&gt;&#xA; RAINBOW$$ using just the rule of &lt;abbr title=&#34;generalized MP&#34;&gt;&lt;a  class=&#34;link-underline-opacity-0 link-body-emphasis&#34;&gt; genMP&lt;/a&gt;&lt;/abbr&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>FOL</title>
      <link>https://logicalmethods.ai/textbook/fol/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://logicalmethods.ai/textbook/fol/</guid>
      <description>&lt;h1 id=&#34;first-order-logic&#34;&gt;First-Order Logic&lt;button class=&#34;btn btn-back-to-top&#34;&gt;&#xA;    &#xA;  &lt;/button&gt;&#xA;&lt;/h1&gt;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&#xA;&lt;img src=&#34;https://logicalmethods.ai/textbook/fol/img/ai_talking_fol.png&#34; class=&#34;rounded  float-end inert-img img-fluid m-2&#34;  width=&#34;200px&#34;  alt=&#34;&#34; &gt;&#xA;&#xA;&lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/First-order_logic&#34;&#xA;  target=&#34;_blank&#34;&gt;First-order logic (FOL)&lt;/a&gt; is&#xA;one of the most powerful logical systems in common use today. Part of its&#xA;strength derives from its &lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Expressive_power_%28computer_science%29&#34;&#xA;  target=&#34;_blank&#34;&gt;expressive&#xA;power&lt;/a&gt;: it&amp;rsquo;s&#xA;ability to express complex ideas in syntactically straightforward and logically&#xA;tractable ways. In fact, FOL-style syntax is the effective paradigm in which&#xA;most of modern mathematics, physics, economics, and other formal theories are&#xA;formulated.&lt;/p&gt;&#xA;&lt;p&gt;This makes reasoning in FOL a natural target for AI research into automated&#xA;reasoning techniques, proof verification, etc. And, in fact, we&amp;rsquo;ll be looking at&#xA;these topics in the &lt;em&gt;next&lt;/em&gt; chapter. In this chapter, we&amp;rsquo;ll explore an even&#xA;deeper connection between FOL and AI, which has to do with the way we formalize&#xA;and store knowledge. We&amp;rsquo;ve already explored the idea of &lt;abbr title=&#34;knowledge bases&#34;&gt;&lt;a  class=&#34;link-underline-opacity-0 link-body-emphasis&#34;&gt; KBs&lt;/a&gt;&lt;/abbr&gt;, which are sets of formulas that we&#xA;use to represent knowledge about the world. When we&amp;rsquo;re using propositional&#xA;logic, these representations are typically rather simplistic, which has to do&#xA;with the lack of expressive power in propositional logic.&lt;/p&gt;</description>
    </item>
    <item>
      <title>FOL Inference</title>
      <link>https://logicalmethods.ai/textbook/fol-inference/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://logicalmethods.ai/textbook/fol-inference/</guid>
      <description>&lt;h1 id=&#34;fol-inference&#34;&gt;FOL Inference&lt;button class=&#34;btn btn-back-to-top&#34;&gt;&#xA;    &#xA;  &lt;/button&gt;&#xA;&lt;/h1&gt;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;It turns out that automating inference in FOL is a &lt;em&gt;hard&lt;/em&gt; problem.&lt;/p&gt;&#xA;&lt;p&gt;Take our standard inference, for example:&lt;/p&gt;&#xA;&#xA;&lt;img src=&#34;https://logicalmethods.ai/textbook/fol-inference/img/socrates_inference.png&#34; class=&#34;mx-auto rounded d-block inert-img img-fluid&#34;  width=&#34;400px&#34;  alt=&#34;&#34; &gt;&#xA;&#xA;&lt;p&gt;It&amp;rsquo;s actually rather straight-forward to see that this inference is deductively&#xA;valid in FOL. Suppose, we&amp;rsquo;ve got a model, $M$, where both premises are true,&#xA;that is:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$M &#xA;&lt;img src=&#34;https://logicalmethods.ai/img/vDash.png&#34; alt=&#34;vDash&#34; class=&#34;inline-icon&#34; style=&#34;height:1em;width:auto;&#34;/&gt;&#xA;&#xA;&lt;img src=&#34;https://logicalmethods.ai/img/forall.png&#34; alt=&#34;forall&#34; class=&#34;inline-icon&#34; style=&#34;height:1em;width:auto;margin-right:-0.4em&#34;/&gt;&#xA;&#xA;x (Human x &#xA;&lt;img src=&#34;https://logicalmethods.ai/img/to.png&#34; alt=&#34;Implication&#34; class=&#34;inline-icon&#34; style=&#34;height:1em;width:auto;&#34;/&gt;&#xA; Mortal x)$&lt;/li&gt;&#xA;&lt;li&gt;$M &#xA;&lt;img src=&#34;https://logicalmethods.ai/img/vDash.png&#34; alt=&#34;vDash&#34; class=&#34;inline-icon&#34; style=&#34;height:1em;width:auto;&#34;/&gt;&#xA; Human Socrates$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;If we unfold the former using the truth-conditions in models, we get:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Many-valued logics</title>
      <link>https://logicalmethods.ai/textbook/many-valued/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://logicalmethods.ai/textbook/many-valued/</guid>
      <description>&lt;h1 id=&#34;many-valued-logics&#34;&gt;Many-valued logics&lt;button class=&#34;btn btn-back-to-top&#34;&gt;&#xA;    &#xA;  &lt;/button&gt;&#xA;&lt;/h1&gt;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&#xA;&lt;img src=&#34;https://logicalmethods.ai/textbook/many-valued/img/ai_warm.png&#34; class=&#34;rounded  float-end inert-img img-fluid m-2&#34;  width=&#34;200px&#34;  alt=&#34;&#34; &gt;&#xA;&#xA;In classical logic, we assume that every formula is either true or false. This&#xA;is known as the &lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Principle_of_bivalence&#34;&#xA;  target=&#34;_blank&#34;&gt;principle of&#xA;bivalence&lt;/a&gt;. While the&#xA;assumption is valid in many contexts, it is clearly an&#xA;&lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Idealization_%28philosophy_of_science%29&#34;&#xA;  target=&#34;_blank&#34;&gt;idealization&lt;/a&gt;&#xA;in others. Think of statements about the future, like &amp;ldquo;it will rain tomorrow&amp;rdquo;.&#xA;Unless you&amp;rsquo;re a die-hard &lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Fatalism&#34;&#xA;  target=&#34;_blank&#34;&gt;fatalist&lt;/a&gt;, you&#xA;probably don&amp;rsquo;t think that statement is true or false &lt;em&gt;now&lt;/em&gt;—its truth or falsity&#xA;will be decided tomorrow. Or think about&#xA;&lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Vagueness&#34;&#xA;  target=&#34;_blank&#34;&gt;vague&lt;/a&gt; concepts, like &lt;em&gt;warmth&lt;/em&gt;. Is&#xA;25°C warm? What about 21, 20, 19°C? Is there a clear cut-off point? If so,&#xA;where? Is it at 20°C, maybe? Does that then mean that 19.999°C is no longer&#xA;warm?—It seems that warmth is something that comes in &lt;em&gt;degrees&lt;/em&gt;: 25°C is &lt;em&gt;warmer&#xA;than&lt;/em&gt; 22°C is warmer than 19°C. Examples like these motivate the study of &lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Many-valued_logic&#34;&#xA;  target=&#34;_blank&#34;&gt;&lt;strong&gt;many-valued&#xA;logics&lt;/strong&gt;&lt;/a&gt;, which abandon&#xA;bivalence.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Probability and inductive logic</title>
      <link>https://logicalmethods.ai/textbook/probability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://logicalmethods.ai/textbook/probability/</guid>
      <description>&lt;h1 id=&#34;probability-and-inductive-logic&#34;&gt;Probability and inductive logic&lt;button class=&#34;btn btn-back-to-top&#34;&gt;&#xA;    &#xA;  &lt;/button&gt;&#xA;&lt;/h1&gt;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&#xA;&lt;img src=&#34;https://logicalmethods.ai/textbook/probability/img/llm_example.png&#34; class=&#34;rounded  float-start inert-img img-fluid m-2&#34;  width=&#34;350px&#34;  alt=&#34;&#34; &gt;&#xA;&#xA;Inductive inference plays a crucial role in AI technologies both on the low&#xA;level and on the high level. On the low level, inductive inference is, for&#xA;example, the logical foundation for &lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Large_language_model&#34;&#xA;  target=&#34;_blank&#34;&gt;large language models&#xA;(LLMs)&lt;/a&gt;, which in turn give&#xA;us &lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Chatbot&#34;&#xA;  target=&#34;_blank&#34;&gt;chatbots&lt;/a&gt; and other modern AI&#xA;technologies. In essence, an LLM is (an approximation of) a &lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Probability_distribution&#34;&#xA;  target=&#34;_blank&#34;&gt;probability&#xA;distribution&lt;/a&gt; over&#xA;&lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Large_language_model#Tokenization&#34;&#xA;  target=&#34;_blank&#34;&gt;tokens&lt;/a&gt;—small-ish&#xA;sequences of characters that make up words. The idea is that we can use the&#xA;&lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Conditional_probability&#34;&#xA;  target=&#34;_blank&#34;&gt;conditional probability&lt;/a&gt;&#xA;of one token given a sequence of others to predict what the &lt;em&gt;next&lt;/em&gt;&#xA;token in the sequence should be. The perhaps surprising fact is, that this&#xA;&amp;ldquo;next-token prediction&amp;rdquo; allows us to develop AI agents with human-like&#xA;abilities.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Logic-based learning</title>
      <link>https://logicalmethods.ai/textbook/learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://logicalmethods.ai/textbook/learning/</guid>
      <description>&lt;h1 id=&#34;logic-based-learning&#34;&gt;Logic-based learning&lt;button class=&#34;btn btn-back-to-top&#34;&gt;&#xA;    &#xA;  &lt;/button&gt;&#xA;&lt;/h1&gt;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;In this, rather short chapter, you&amp;rsquo;ll learn about how we handle &lt;em&gt;learning&lt;/em&gt; on a&#xA;logic-based approach to AI.&lt;/p&gt;&#xA;&lt;p&gt;We&amp;rsquo;ll discuss two approaches:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;a &lt;em&gt;deductive&lt;/em&gt; logic-based approach, known as &#xA;&lt;a class=&#34;link-dark&#34; href=&#34;#belief-revision&#34;&gt;belief&#xA;revision&lt;/a&gt;, and&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;an &lt;em&gt;inductive&lt;/em&gt;, probability-based approach, known as &#xA;&lt;a class=&#34;link-dark&#34; href=&#34;#bayesian-updating&#34;&gt;Bayesian updating&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Both approaches find applications in AI, even though Bayesian updating is &lt;em&gt;by&#xA;far&lt;/em&gt; more popular. In the end, though, symbolic learning methods are &lt;em&gt;not&lt;/em&gt; the&#xA;gold standard for &lt;i class=&#34;bi bi-box-arrow-up-right h-6&#34;&gt;&lt;/i&gt;&#xA;&lt;a class=&#34;link-dark&#34; href=&#34;https://en.wikipedia.org/wiki/Machine_learning&#34;&#xA;  target=&#34;_blank&#34;&gt;machine&#xA;learning&lt;/a&gt;—statistics is. In this&#xA;chapter, we&amp;rsquo;ll see why.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
