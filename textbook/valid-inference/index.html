<!doctype html>
<html lang="en" data-bs-theme="light">
  <head>
    <meta charset="utf-8"></meta>
    <meta name="viewport" content="width=device-width, initial-scale=1"></meta>
    <title>logicalmethods.ai &ndash; Valid Inference </title>
    <link rel="icon" type="image/x-icon" href="https://logicalmethods.ai/favicon.ico"></link>
    <link href="https://logicalmethods.ai/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet"></link>
    <link rel="stylesheet" href="https://logicalmethods.ai/icons/font/bootstrap-icons.min.css"></link>
    <link rel="stylesheet" href="https://logicalmethods.ai/css/layout.css"></link>
    <link href="https://logicalmethods.ai/css/textbook.css" rel="stylesheet"></link>
    <style>
      
    :root {
      --chapter: "2";
    }

    </style>
    
      <link rel="stylesheet" href="https://logicalmethods.ai/katex/katex.min.css">
<script defer src="https://logicalmethods.ai/katex/katex.min.js"></script>
<script defer src="https://logicalmethods.ai/katex/contrib/auto-render.min.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},     
        {left: '$', right: '$', display: false},     
        {left: '\\[', right: '\\]', display: true},   
        {left: '\\(', right: '\\)', display: false},  
      ],
      throwOnError : false
    });
  });
</script>


    
  </head>
  <body class="bg-black d-flex flex-column m-0 p-0">
    <div class="offcanvas offcanvas-start" tabindex="-1" id="offcanvas-nav" aria-labelledby="offcanvas-navLabel" data-bs-theme="dark">
  <div class="offcanvas-header">
    <div class="h5 offcanvas-title" id="offcanvas-navLabel">
      Browse course
    </div>
    <button type="button" class="btn-close" data-bs-dismiss="offcanvas" aria-label="Close"></button>
  </div>
  
  <div class="offcanvas-body">
    <ul class="navbar-nav"><li class="nav-item">
        <div class="d-flex flex-row">
          <div class="container">
            <a class="nav-link fw-bold active" href="/textbook/"><i class="bi bi-book"></i> &nbsp;&nbsp; Textbook</a>
          </div>
          <button 
            class="btn btn-sm btn-dark" 
            href="/textbook/"
            id="navbarSectiontxt-home" data-bs-toggle="collapse"
            data-bs-target="#collapsetxt-home"  
            aria-expanded="true"
            aria-controls="collapsetxt-home">
            <i class="bi bi-caret-left-fill toggle-icon"></i>
          </button>
        </div>
        <div class="collapse show" id="collapsetxt-home">
          <ul class="list-unstyled ms-3">
            <li>
              <a class="dropdown-item " href="/textbook/logic-and-ai/">1. Logic and AI</a>
            </li>
            
            <li>
              <a class="dropdown-item fw-bold active" href="/textbook/valid-inference/">2. Valid Inference</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/textbook/formal-languages/">3. Formal languages</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/textbook/boolean/">4. Boolean algebra</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/textbook/sat/">5. Boolean satisfiability</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/textbook/conditionals/">6. Logical conditionals</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/textbook/proofs/">7. Logical proofs</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/textbook/fol/">8. FOL</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/textbook/fol-inference/">9. FOL Inference</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/textbook/many-valued/">10. Many-valued logics</a>
            </li>
            </ul>
        </div>
        
      </li>
      <hr>
      <li class="nav-item">
        <div class="d-flex flex-row">
          <div class="container">
            <a class="nav-link " href="/slides/"><i class="bi bi-rocket-takeoff-fill"></i> &nbsp;&nbsp; Slides</a>
          </div>
          <button 
            class="btn btn-sm btn-dark" 
            href="/slides/"
            id="navbarSectionsli-home" data-bs-toggle="collapse"
            data-bs-target="#collapsesli-home"  
            aria-expanded="aria-expanded=false"
            aria-controls="collapsesli-home">
            <i class="bi bi-caret-left-fill toggle-icon"></i>
          </button>
        </div>
        <div class="collapse " id="collapsesli-home">
          <ul class="list-unstyled ms-3">
            <li>
              <a class="dropdown-item " href="/slides/logic-and-ai/">1. Logic and AI</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/slides/valid-inference/">2. Valid inference</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/slides/formal-languages/">3. Formal languages</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/slides/boolean/">4. Boolean algebra</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/slides/sat/">5. Boolean satisfiability</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/slides/conditionals/">6. Logical conditionals</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/slides/proof/">7. Logical proofs</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/slides/fol/">8. FOL</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/slides/fol-inference/">9. FOL inference</a>
            </li>
            </ul>
        </div>
        
      </li>
      <hr>
      <li class="nav-item">
        <div class="d-flex flex-row">
          <div class="container">
            <a class="nav-link " href="/exercises/"><i class="bi-gear-fill"></i> &nbsp;&nbsp; Exercises</a>
          </div>
          <button 
            class="btn btn-sm btn-dark" 
            href="/exercises/"
            id="navbarSectionexe-home" data-bs-toggle="collapse"
            data-bs-target="#collapseexe-home"  
            aria-expanded="aria-expanded=false"
            aria-controls="collapseexe-home">
            <i class="bi bi-caret-left-fill toggle-icon"></i>
          </button>
        </div>
        <div class="collapse " id="collapseexe-home">
          <ul class="list-unstyled ms-3">
            <li>
              <a class="dropdown-item " href="/exercises/logic-and-ai/">1. Logic and AI</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/exercises/valid-inference/">2. Valid inference</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/exercises/formal-languages/">3. Formal languages</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/exercises/boolean/">4. Boolean algebra</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/exercises/sat/">5. Boolean satisfiability</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/exercises/conditionals/">6. Logical conditionals</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/exercises/proof/">7. Logical proofs</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/exercises/fol/">8. FOL</a>
            </li>
            
            <li>
              <a class="dropdown-item " href="/exercises/fol-inference/">9. FOL inference</a>
            </li>
            </ul>
        </div>
        
      </li>
      <hr>
      <li class="nav-item">
        <div class="d-flex flex-row">
          <div class="container">
            <a class="nav-link " href="/about/"><i class="bi-file-person-fill"></i> &nbsp;&nbsp; About</a>
          </div>
      </li>
      <hr>
      
    </ul>
  </div>
</div>

    <header class="align-self-center d-flex flex-column m-0 mb-md-1 p-0" data-bs-theme="dark">
      <nav class="navbar bg-dark shadow" data-bs-theme="dark" aria-label="header-navbar">
  <div class="container-sm align-self-center px-3">
    <a class="navbar-brand fs-6" href="https://logicalmethods.ai/"><span style="margin-right:-4px;vertical-align: middle;transform-origin: 50%  49%;transform:rotate(180deg);display:inline-block">IA</span>
<i class="bi bi-caret-right-fill"></i>logicalmethods.ai</a>
    <button class="navbar-toggler border-0" type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas-nav" aria-controls="offcanvas-nav">
      <span class="navbar-toggler-icon"></span>
    </button>
  </div>
</nav>

      
      
    </header>
    
    <main class="align-self-center d-flex flex-column flex-fill bg-light m-md-1 p-0" data-bs-theme="light">
      <ul class="navbar-nav flex-row justify-content-between mt-2">
  
  <li class="nav-item">
    <a href="/textbook/logic-and-ai/" class="btn" tabindex="-1" role="button" aria-disabled="true">
      <i class="bi bi-chevron-left"></i>
    </a>
    
  </li>
  <li class="nav-item">
    <a href="/textbook/" class="btn " tabindex="-1" role="button" aria-disabled="true">
      <i class="bi bi-house-up"></i>
    </a>
  </li>
  <li class="nav-item">
    
    <a href="/textbook/formal-languages/" class="btn" tabindex="-1" role="button" aria-disabled="true">
      <i class="bi bi-chevron-right"></i>
    </a>
    
  </li>
</ul>
<div class="container chapter">
  <p>By: <em>Colin Caret</em></p><h1 id="valid-inference">Valid inference<button class="btn btn-back-to-top">
    
  </button>
</h1>
<hr>

<p>Remember from 


<a href="https://logicalmethods.ai/textbook/logic-and-ai/"
   
   
   
   
   
   >
Chapter 1. Logic and AI</a>


that logic is the study of <strong>valid inference</strong>. In this
chapter, you&rsquo;ll learn more about the <em>concept</em> of validity.</p>
<p>In particular, we&rsquo;ll go into more details of what it means for an inference to
be <a href="#correctness">correct</a>, we&rsquo;ll describe <a href="#formalisation">abstract methods</a>
for modeling valid inferences, and we&rsquo;ll dig into important differences
between <a href="#deduction">deductive</a> and <a href="#induction">inductive validity</a>.</p>
<h2 id="correctness">Correctness<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h2>
<hr>

<p>When we talk about validity, we are talking about a <em>good feature</em> of inferences,
but this is not the only good feature an inference can have. For example, it is
good for inferences to be simple, clear, precise, economical, etc. Logic does
not deal with all of these topics. Most of these topics are part of <strong>rhetoric</strong>,
the study of persuasive writing style. Logic deals with <em>validity</em>.</p>
<p>Validity is a standard of <strong>correctness</strong> for inferences. To really fix this
concept, it might help to think about it from the other direction. What happens
if an inference <em>lacks</em> validity, when it is <strong>invalid</strong>? Well, that shows us
that something went wrong. The inference made a <em>mistake</em>. As we mentioned in



<a href="https://logicalmethods.ai/textbook/logic-and-ai//#validity"
   
   
   
   
   
   >
Chapter 1.1.2 Validity</a>

, some of these logical mistakes or
<strong>fallacies</strong> are so famous that they have their own names.</p>
<p>Here is an example of a fallacy called <em>affirming the consequent</em>:</p>
<ol>
<li>If it is sunny, Jan is cycling. Jan is cycling. Therefore, it is sunny.</li>
</ol>
<p>The mistake should be clear. The <em>conditional</em> premise (&ldquo;if&hellip;, then&hellip;&rdquo;) says
that sun leads to sport. However, the other premise of this inference is not
<em>about</em> sunny weather, so those two premises don&rsquo;t really &ldquo;add up&rdquo; to anything
useful. They certainly do not support the conclusion that it is sunny today.</p>
<p>Here is an example of a valid inference called &ldquo;modus ponens&rdquo;.</p>
<ol start="2">
<li>If it is sunny, Jan is cycling. It is sunny. Therefore, Jan is cycling.</li>
</ol>
<p>This inference involves a premise about sunny weather, which connects with the
conditional premise in the right way. This inference is valid because it uses
all of its premises correctly. These two examples show that it can be easy to
confuse valid and invalid inferences if we don&rsquo;t pay attention to the details.
At a glance, the two inferences look pretty similar.</p>
<p>This is why logic aims for a <em>systematic</em> definition of validity. This notion
should be applicable not only to humans but to any information-processing
system. It can tell us what counts as <em>intelligent</em> behavior. Without a
definition of correct reasoning, how do we even know what we want AI to achieve?
So, we would like to say, in general, what <em>makes</em> an inference valid or
invalid. The standard idea is that valid inferences <strong>preserve truth</strong> from
their premises to their conclusion.</p>
<h3 id="hypothetically">Hypothetically<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h3>
<hr>

<p>To test whether an inference is valid, we ask if the conclusion is true <em>when</em>
the premises are true. This is a <strong>hypothetical</strong> question. Answering this
question does not require us to know that the premises really <em>are</em> true. In
fact, we can make a stronger point: it is possible to have an inference that is
valid even though it has false premises. Here is an example.</p>
<ol start="3">
<li>All pigs fly. Maddy is a pig. So, Maddy flies.</li>
</ol>
<p>The <em>reasoning</em> behind this inference is perfectly correct. The conclusion
follows from the premises. That makes the inference valid. But we obviously know
that this inference also involves some false premises. Pigs can&rsquo;t really fly.</p>
<p>The important point is that validity is not determined by the actual truth or
falsity of statements. What we care about is the <em>connections between</em>
statements.</p>
<p>When we test for validity, we do not look at the actual truth of premises and
conclusion, instead we look for a <strong>relationship</strong> between their truth-values.
You can think about it like this: imagine that the premises and true and think
about whether the conclusion is true <em>under this assumption</em>.</p>
<p>Think about inference (3) this way. When we do this, we imagine a world that is
slightly different from the actual world, a world where pigs do fly. In that
kind of world, it has to be true that Maddy flies.</p>
<p>This is the basic idea of truth-preservation.</p>
<ul>
<li><strong>Valid Inference:</strong> an inference whose conclusion is true under the
(hypothetical) assumption that all of the premises are true.</li>
</ul>
<p>Now, we can ask a series of follow-up questions. How tight is the
truth-preservation relationship? How often does it have to hold? How reliable
does an inference need to be in order to call it &ldquo;valid&rdquo;? Different answers to
these questions take us in two directions: deductive and inductive logic.</p>
<h3 id="always">Always<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h3>
<hr>

<p>With <strong>deductively valid</strong> inference, truth-preservation <em>always</em> holds. We want
100% reliability in all situations whatsoever. No exceptions allowed.
<em>Necessarily</em>, if the premises are true, the conclusion is true. Otherwise, its
not deductively valid. This is a high standard we are asking for but it is very
nice when we can identify this kind of air-tight reasoning.</p>
<p>Here are some common examples of deductively valid inferences.</p>
<ol start="4">
<li>
<p>If this superintelligent AI system is dangerous, then it gives a lot of bad
advice. It does not give a lot of bad advice. So, this superintelligent AI
system is not dangerous.</p>
</li>
<li>
<p>Either it rained last night or the car is dry. The car is not dry. Thus, it
rained last night.</p>
</li>
<li>
<p>All dogs are friendly. Some dogs are chubby. So, some chubby animals are
friendly.</p>
</li>
</ol>
<p>In deductive logic, we use the symbol
$$\vDash$$
to stand for a deductively valid inference. So if we have a deductively valid
inference going from premise $P_1,P_2,\dots$ to conclusion $C$, we can
abbreviate this with symbols:</p>
<p>$$P_1,P2,\mathellipsis \vDash C$$</p>
<p>In inference 5, for example, we have that $P_1$ is the sentence &ldquo;Either it
rained last night or the car is dry&rdquo;, $P_2$ is &ldquo;The car is not dry&rdquo;, and $C$ is
&ldquo;It rained last night&rdquo;. So, we can write the inference as:</p>
<p>$$\text{Either it rained last night or the car is dry}, \text{The car is not
dry}\vDash \text{It rained last night}$$</p>
<h3 id="mostly">Mostly<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h3>
<hr>

<p>With <strong>inductively valid</strong> inference, truth-preservation <em>mostly</em> holds. If the
premises are true, the conclusion is <em>probably</em> true. The likelihood could be
slightly increased (weak induction) or greatly increased (strong induction).
Either way, assuming that the premises are true gives us <em>some</em> reason to
believe the conclusion is true. Inductive inference is not air-tight. There is a
chance of going from truth to falsity, but that is a risk we have to take when
we are dealing with uncertain information.</p>
<p>Here are some common examples of inductively valid inferences. In each of these
examples, the premises give us good reason to believe the conclusion, but <em>none</em>
of these &lsquo;risky&rsquo; inferences qualifies as a deductively valid inference (all of
them are deductively invalid).</p>
<ol start="7">
<li>
<p>It looks like a duck. It quacks like a duck. So, it is a duck.</p>
</li>
<li>
<p>Strict AI laws have a support of around 80% in a randomly selected sample of
20.000 voters. Therefore, support for strict AI laws in the general public is
around 80%.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/GPT-4"
  target="_blank">GPT-4</a> improved upon
<a href="https://en.wikipedia.org/wiki/GPT-3"
  target="_blank">GPT-3</a>, which improved upon
<a href="https://en.wikipedia.org/wiki/GPT-2"
  target="_blank">GPT-2</a>, which, in turn, improved upon
<a href="https://en.wikipedia.org/wiki/GPT-1"
  target="_blank">GPT-1</a> in terms of coherence and
relevance. Therefore, the next generation of GPT models will further improve in
this respect.</p>
</li>
</ol>
<p>In inductive logic, we use the symbol $$\mid\approx$$ to stand for an
inductively valid inference, writing $$P_1, P_2, \dots\mid\approx C$$ to say
that the inference from premises $P_1, P_2, \mathellipsis$ to conclusion $C$ is
inductively valid. To say that an inference is inductively <em>strong</em>, we write a
$!$ on top, like so: $$\stackrel{!}{\mid\approx}$$. So if we have a inductively
strong inference going from premises $P_1, P_2, \dots$ to conclusion $C$ we can
abbreviate this with symbols: $$P_1, P_2, \dots\stackrel{!}{\mid\approx} C.$$</p>
<h2 id="formalization">Formalization<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h2>
<hr>

<p>By steps, we can make things even more precise. One of the steps we mentioned in



<a href="https://logicalmethods.ai/textbook/logic-and-ai/"
   
   
   
   
   
   >
Chapter 1. </a>

 is <strong>formalization</strong>. We can build
formal languages and study their properties. We can also develop theories about
how these kind of formal patterns map onto ordinary human communication and
thinking.</p>
<p>One powerful idea in logic is that different inferences have the same <strong>logical
form</strong>. This allows us to identify many valid inferences all at once, just by
studying their shared form. Reasoning that can be captured in a finite set of
symbols and rules can also be programmed into a computer. This was essentially
the idea that sparked the first wave of AI research in the 1950s.</p>
<p>The next chapter will delve into more details of formal syntax and how to
interpret the logical forms of sentences. We will mention just a few examples of
formalisation that are useful for this chapter.</p>
<ul>
<li>
<p>The symbol &ldquo;$\neg$&rdquo; can be read &ldquo;not&rdquo; in English. $\neg A$ says &ldquo;Not $A$.&rdquo;</p>
</li>
<li>
<p>The symbol &ldquo;$\land$&rdquo; can be read &ldquo;and&rdquo; in English. $A\land B$ says &ldquo;Both $A$
and $B$.&rdquo;</p>
</li>
<li>
<p>The symbol &ldquo;$\rightarrow$&rdquo; can be read &ldquo;if, then&rdquo; in English. $A\rightarrow B$
says &ldquo;If $A$, then $B$&rdquo;.</p>
</li>
</ul>
<p>Let&quot;s see how this formalisation is used:</p>
<ol>
<li>
<p>The inference form of &ldquo;modus ponens&rdquo; looks like this: $ A\rightarrow B,
A\vDash B$. This sequence of symbols says that whenever you put together two
separate pieces of information, a premise &ldquo;If $A$, then $B$&rdquo; and a premise $A$,
you can validly infer the conclusion $B$. We said that example 2. follows this
pattern. Here are some more examples. These are all instances of &ldquo;modus ponens&rdquo;:</p>
<ol start="10">
<li>
<p>If programming is difficult to learn, then some people cannot do it.
Programming is difficult to learn. Therefore, some people cannot do it.</p>
</li>
<li>
<p>If ELIZA ever beat GPT-3.5 in the Turing Test, then it is cutting-edge AI
technology. <a href="https://arxiv.org/abs/2310.20216"
  target="_blank">ELIZA did beat GPT-3.5</a> in the
Turing Test. So, ELIZA is cutting-edge AI technology.</p>
</li>
<li>
<p>If the sun will rise tomorrow, then you will win the lottery next week. The
sun will rise tomorrow. So, you will win the lottery next week.</p>
</li>
</ol>
<p>In logic, we represent inferences 10.-12. the same way. They have the same
form but different content. As we will show later, the form is enough to
explain why all of these inferences are valid. Obviously, this does not
guarantee that the conclusion is actually true!</p>
</li>
<li>
<p>The fallacy of &ldquo;affirming the consequent&rdquo; looks like this: $ A\rightarrow B,
B\nvDash A$. This sequence of symbols says that if you put together two separate
pieces of information, a premise &ldquo;If $A$, then $B$&rdquo; and a premise $B$, it is
invalid to infer the conclusion $A$. If you use this reasoning, you are making a
mistake. We said that example 1. commits this mistake. Here are some more
examples:</p>
<ol start="13">
<li>
<p>If ZFC is consistent, it cannot settle the value of BB(8000). ZFC cannot
<a href="https://scottaaronson.blog/?p=2725"
  target="_blank">settle the value of BB(8000)</a>. So, ZFC is
consistent.</p>
</li>
<li>
<p>If Strong AI is possible, then there is a flaw in the <a href="https://plato.stanford.edu/entries/chinese-room/"
  target="_blank">Chinese Room thought
experiment</a>. There is a flaw
in the Chinese Room thought experiment. Therefore, Strong AI is possible.</p>
</li>
<li>
<p>If Topsy was an elephant, then Topsy had a trunk. Topsy had a trunk. So,
<a href="https://en.wikipedia.org/wiki/Topsy_%28elephant%29"
  target="_blank">Topsy was an elephant</a>.</p>
</li>
</ol>
<p>Again, we can represent 13.-15. the same way and this common structure can
explain why all of these inferences are invalid. This means that they use
reasoning. It does not necessarily mean that all of these inferences have
false conclusions! For example, the conclusion of 15. is true, but it would
be a mistake if you came to believe this conclusion by using this fallacious
reasoning.</p>
</li>
</ol>
<p>We have now said a lot about the concept of validity. But what do the tools of
logical analysis look like? How do we use logical form to identify <em>whether an
inference is valid</em>? That is our next topic.</p>
<h2 id="deduction">Deduction<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h2>
<hr>

<p>The study of deduction goes back to ancient philosophy and mathematics.
<a href="https://plato.stanford.edu/entries/aristotle-logic/"
  target="_blank">Aristotle</a> considered it
to be the pinnacle of reasoning.
<a href="https://en.wikipedia.org/wiki/Euclidean_geometry"
  target="_blank">Euclid</a> used deductive
reasoning to prove things about basic geometry. He also introduced the
<strong>axiomatic method</strong>. This method has two parts: axioms and rules. An axiom is
just a definition.</p>
<p>The first Euclidan axiom says &ldquo;between any two points there is a line&rdquo;, which
partly defines the abstract concepts of <em>point</em> and <em>line</em> by telling us how
these things relate to each other. The other part of this method is a set of
rules for reasoning. In other words: logic.</p>
<p>Rules for deductive logic are <em>indefeasible</em> and suited to belief
<em>accumulation</em>. These rules do not vary between contexts. They do not require
special justification. There is nothing that can make these rules fail. Nothing
can defeat a deductively valid inference.</p>
<p>The optimal way of using deduction is to take a set of existing beliefs (or
knowledge) and then add more beliefs (or knowledge) by applying logical rules. A
common, real world <em>implementation</em> of deductive reasoning occurs whenenever we
apply some general pattern to a specific case:</p>
<ol start="16">
<li>
<p>Vixens are female foxes. That is the definition of the concept <em>vixen</em>.
Suppose you know this and you hear someone say &ldquo;there is a vixen living in the
forest!&rdquo;. In that case, you might use deduction to infer that there is a fox
living the forest.</p>
</li>
<li>
<p>Suppose that you live in a village where the bus does not run on days when
there is a football match. You know about this policy. You also know that there
is a football match today. If you put together these existing beliefs, you might
use deduction to infer that there is no bus running today.</p>
</li>
</ol>
<p>Like the examples of &ldquo;modus ponens&rdquo;, we have here two examples of inferences
with the same logical form. It is actually closely related to &ldquo;modus ponens&rdquo; but
also involves the <strong>quantifier</strong> &ldquo;all&rdquo; or &ldquo;every&rdquo;. In example 16. there is an
implicit premise &ldquo;all vixens are female foxes&rdquo;. In example 17. there is an
implicit premise &ldquo;all days with footbal are days without the bus&rdquo;.</p>
<p>Both of these inferences simply apply general pattern to a specific instance.
That reasoning is correct as long as the  word &ldquo;all&rdquo; <em>really</em> means &ldquo;all&rdquo;. If we
assume that a totally general pattern is true &ldquo;all Ps are Qs&rdquo; and &ldquo;this is P&rdquo; is
also true, it has to be true that &ldquo;this is Q&rdquo;. The conclusion follows
necessarily.</p>
<p>The concept of deductive validity is specialized. It makes sense to use
deductive reasoning for specific tasks: reasoning in mathematics or other
axiomatic theories, reasoning with precisely defined concepts or pattern that
are truly general. Notice how this air-tight reasoning is not quite the same
thing as what Sherlock Holmes calls &ldquo;doing a deduction&rdquo;:</p>
<div class="container " >
  <figure>
    <blockquote class="blockquote fs-6">
      <p>
      “From a drop of water&hellip; a logician could infer the possibility of an Atlantic
or a Niagara without having seen or heard of one or the other. So all life is
a great chain, the nature of which is known whenever we are shown a single
link of it. Like all other arts, the Science of Deduction and Analysis is one
which can only be acquired by long and patient study&hellip;&quot;
      </p>
    </blockquote>
    
    <figcaption class="blockquote-footer text-end">
      A study in scarlet, 1887
    </figcaption>
    
  </figure>
</div>

<p>Sherlock uses the word &ldquo;deduction&rdquo; for any reasoning that is careful,
systematic, and reliable. However, the examples in this passage sounds a lot
like inductive reasoning. Take a small sample and make an educated guess about
the larger collection that it belongs to. That kind of reasoning is not
indefeasible, it does not make a necessary connection. <em>We</em> do not call that
deduction.</p>
<p>Many <strong>programming languages</strong> are essentially formal languages with deductive
rules. This is a perfect way to apply logical methods. Consider the code snippet
below. This code is supposed to take an input that is a whole number and
identify whether it is a positive number or not:</p>
<pre tabindex="0"><code>num = int(input(&#39;Enter a whole number: &#39;))

if num &gt; 0:
     print(f&#39;{num} is positive.&#39;)
else:
     print(f&#39;{num} is not positive.&#39;)
</code></pre><p>Imagine what would happen if the computer did, effectively, not obey deductive
rules like &ldquo;modus ponens&rdquo;. We would have no idea what to expect when we run this
program. Sometimes when you ran the program and entered the number 1, you might
get the correct answer that 1 is a positive number, but other times you might
you get no answer at all. That would be useless and frustrating. In order to
make the behavior of programs predictable, we want them to effectively follow
deductive rules of reasoning.</p>
<h3 id="semantic-methods-for-deduction">Semantic methods for deduction<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h3>
<hr>

<p>Let&rsquo;s now take a brief look at <strong>semantics</strong>. This is the part of logic where we
give a model of meaning and truth. A semantics for a language delineates which
inferences are deductively valid in that language. To keep things simple we will
talk about a stripped-down formal language that only represents the logical form
of &ldquo;and&rdquo; sentences like &ldquo;$A$ and $B$&rdquo;, represented $A\land B$.</p>
<p>Even with a stripped-down formal language like this, we can learn a lot about
deductive logic. The basic tool we will use is called a <strong>semantic model</strong>. A
semantic model is like a picture of a possible reasoning scenario. If we look at
a model, we can ask whether a specific sentence $A$ is true or false in that
model. Every model gives an answer. It assigns a definite truth-value to each
sentence.</p>
<p>A formal language $\mathcal{L}$ has more than one model. Different models
assign different values to the same sentences. In one model perhaps $A$ is
true and $B$ is false. In another model $A$ is false and $B$ is
true. That variation allows models to picture different scenarios. Once we have
defined all possible models for a language $\mathcal{L}$, how do we use
them?<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>For any sentence $A$ in our formal language, we will write $[A]$ to
refer to the set of models where that sentence is true. With this notation we
can talk about relations between sets of models $[A]$ and $[B]$ for any
two sentences $A$ and $B$. When the elements of one set are also
contained in another set, we say that there is a <strong>subset</strong> relation between
them.</p>
<p>For example, the set of triangle is a subset of the set of figures. The set of
cats is a subset of the set of animals. All of the things in the first set are
also in the second set. If $[A]$ is a subset of $[B]$, we write
$[A]\subseteq[B]$.</p>
<p>This relation is a good first step to defining deductive validity:<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
<p>$$A\vDash B \text{ if, and only if, } [A]\subseteq[B]$$</p>
<p>Why does this definition work? Let&rsquo;s analyze what this says. In the background
we assume that there is well-defined notion of all possible models for our
language. From there, we define $[A]$ and $[B]$ for any two sentences
$A$ and $B$. Suppose that $[A]\subseteq[B]$ holds. That means for
<em>absolutely any</em> model $\mathcal{M}$ of our language, if $\mathcal{M}$
makes $A$ true, then $\mathcal{M}$ also makes $B$ true. That means
that an inference from premise $A$ to conclusion $B$ would preserve
truth in every possible scenario. It would <em>always</em> preserve truth. It is
<em>necessarily</em> truth-preserving. And that is exactly the concept of <strong>deductive
validity</strong> $\vDash$.</p>
<p>The approach we have just described is very abstract. It can be used for <em>any</em>
formal language. If we can explain how to define all possible models for a
language, the definition of deductive validity just &lsquo;falls out&rsquo; of that
collection of semantic models.</p>
<p>To give an illustration of this method, let&rsquo;s look at some inferences with &ldquo;and&rdquo;
sentences. When we write $A\land B$ it is supposed to represent a single
complex sentence that combines two simpler thoughts. For example, we could
formalise English sentences like this.</p>
<ul>
<li>
<p>$P$ formalises &ldquo;Putnam is happy.&rdquo;</p>
</li>
<li>
<p>$Q$ formalises &ldquo;Quine is happy.&rdquo;</p>
</li>
<li>
<p>$P\land Q$ formalises &ldquo;Putnam and Quine are both happy.&rdquo;</p>
</li>
</ul>
<p>If a person asserts $P$ and then asserts $Q$, they make two separate
statements. If a person asserts $P\land Q$, they make one single statement
that has a complex, inner structure made from a combination of simpler
statements with the logical word &ldquo;and&rdquo;. Semantics can model the meaning of this
logical word as an operation on sets of models. The relevant operation is called
<strong>set intersection</strong>:</p>


<img src="https://logicalmethods.ai/textbook/valid-inference/img/intersection.jpeg" class="img-thumbnail"  alt="Intersection" >


<p>In the diagram, we see that there is a set $X$ and there is a set $Y$.
Maybe these are sets of people or sets of numbers. It doesn&rsquo;t really matter. We
just want to see what the intersection of two sets refers to. The intersection
$X\cap Y$ is the overlapping part of those two sets. The elements of
$X\cap Y$ are all of the things that are shared in common between $X$
and $Y$.</p>
<p>For example, if $F$ was the set of all fruits and $O$ was the set of all
orange things, then $F\cap O$ would be the set of orange fruits. This
intersection set contains mandarines, apricots, and mangoes but it does not
contain bananas (not orange) and it does not contain carrots (not fruits).</p>
<p>We can apply this operation to any two sets. What if we apply it to $[A]$
and $[B]$? What does $[A]\cap[B]$ refer to? For <em>absolutely any</em> model
$\mathcal{M}$ in the intersection set $[A]\cap[B]$, that same model
$\mathcal{M}$ has to make $A$ true and $\mathcal{M}$ has to make
$B$ true. This operation essentially takes the models of two separate
sentences and cuts them down to the set of models that make both true at once.
It is natural to identify the meaning of &ldquo;and&rdquo; with this operation: $$[A\land
B] = [A]\cap[B].$$ This gives a very precise meaning to &lsquo;$\land$&rsquo; which we
call the <strong>logical conjunction</strong>.</p>
<p>Using these methods, we can establish some useful facts about valid inferences.
The way we verify these facts is to just think carefully about the definitions
of formal symbols like &lsquo;$\land$&rsquo; and &lsquo;$\vDash$&rsquo;. In other words, we are going to
put the axiomatic approach into action. We take definitions of basic concepts
and see where they lead. In this chapter, we will only sketch the explanations
or proofs of these facts.</p>
<ul>
<li>$A\land B \vDash A$</li>
</ul>
<p>Why is this form of inference valid?</p>
<p><em>Sketch of an Explanation</em>: $[A\land
B]=[A]\cap[B]$ and since the intersection of two sets has to be part of each
set, we know that $[A]\cap[B]\subseteq[A]$.</p>
<ul>
<li>If $A \vDash B$ and $A \vDash C$, then $A \vDash B\land C$</li>
</ul>
<p>This says that if there are two valid inferences using the same premises, you
can also use those premises to validly infer a conjunction of the previous
conclusions.</p>
<p><em>Sketch of an Explanation</em>: Assuming that $A \vDash B$ and $A
\vDash C$ hold we have $[A]\subseteq[B]$ and $[A]\subseteq[C]$. So if
any model $\mathcal{M}$ is in $[A]$, the same  model $\mathcal{M}$
also has to be in $[B]\cap[C]$. We have $[A]\subseteq[B]\cap[C]$.</p>
<ul>
<li>If $A \vDash C$, then $A\land B \vDash C$</li>
</ul>
<p>This principle says that if it is valid to infer a conclusion from some
premises, then it is valid to infer the exact same conclusion after &lsquo;adding new
information&rsquo; to those premises.</p>
<p><em>Sketch of an Explanation</em>: We showed that $A\land B \vDash A$. So if we
asume $A \vDash C$ then we have both $[A\land B]\subseteq[A]$ and
$[A]\subseteq[C]$. This implies $[A\land B]\subseteq[C]$ because the
subset relation is <strong>transitive</strong>: if first set is part of second, and second
set is part of third, the first one has to be part of the third.</p>
<p>The last fact is called <strong>monotonicity</strong> and it gets to the heart of what makes
deduction special. If there is an element of your beliefs that implies a
conclusion, then as long as you hold on to that original element your beliefs
will always imply the same conclusion &ndash; no matter how much new information you
aquire! This is why we say that deductive rules are <em>indefeasible</em>.</p>
<p><a href="https://en.wikipedia.org/wiki/John_McCarthy_%28computer_scientist%29"
  target="_blank">John
McCarthy</a> was
famously optimistic about the power of logical methods. He even thought we could
use them to capture the kind of <em>commonsense reasoning</em> that most adults are
capable of doing. This led to the field of <strong>knowledge representation</strong> where
engineers try to formalise vast amounts of information. These are like axioms of
a massively complicated theory, but the method has its limitations.</p>
<h2 id="induction">Induction<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h2>
<hr>

<p>The study of induction also has roots in ancient philosophy and psychology. The
Buddhist scholars, and brothers, <a href="https://plato.stanford.edu/entries/logic-india/"
  target="_blank">Asaṅga and
Vasubandhu</a> focused on
inferences like &ldquo;where there is smoke, there is fire&rdquo;. They considered
associative thinking, tracking regularities in experience, and drawing
structural analogies to be the cornerstones of real human belief-forming
practices. This was an attempt to describe how the mind really functions to help
us navigate a world where evidence is imperfect.</p>
<p>Rules for inductive logic are <strong>defeasible</strong> and suited to <strong>belief
modulation</strong>. These rules can vary between contexts. They often require special
backing. These rules can fail, or perhaps a better way to put it is that an
inductive rule can be better in some environments, worse in others.</p>
<p>The early work on logic-based AI quickly shifted from deductive logic to
inductive logic. The main reason is that induction is immensely important to a
complete description of commonsense reasoning.</p>
<p>Suppose that you agreed to meet Karl for lunch in a few minutes. Karl eats lunch
every day in the canteen and he always gets there early. You infer that Karl is
in the canteen now. This is inductively valid. However, as you are walking to
the canteen you see that the building is on fire and the occupants are standing
on the pavement watching the fire fighters put out the blaze. You <em>take back</em>
the inference and the conclusion you previously drew. Now, you infer that Karl
is not in the canteen (at least, you hope not).</p>
<p>The optimal way of using induction is to take a set of existing beliefs (or
knowledge) and then tentatively entertain the beliefs (or knowledge) that are
most likely to be true based on your initial information. Some common, real
world <em>implementations</em> of inductive reasoning occur when we use observations of
similarities to infer the existence of a general pattern, or we apply so-called
<em>generic</em> information.</p>
<ol start="18">
<li>
<p>I tallied thousands of swans around the river Rhine. I tallied thousands of
swans around the IJsselmeer. Every swan that I observed was white. So, I infer
that all swans are white.</p>
</li>
<li>
<p>Birds fly. Tweety is a bird. Thus, tweety flies.</p>
</li>
</ol>
<p>The example of Tweety is a famous example of perfectly good induction, but one
that obviously &lsquo;goes wrong&rsquo; in some contexts. Generic information like &ldquo;birds
fly&rdquo; is very different from a truly general pattern. It is not talking about
<em>absolutely every</em> bird. It means that most birds fly, typical birds fly. So if
you apply this generic information to Tweety, then your conclusion is probably
true. But this conclusion could be false if it turns out that Tweety is a
penguin or an ostrich.</p>
<h3 id="semantic-methods-for-induction">Semantic methods for induction<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h3>
<hr>

<p>One way to approach semantics for inductive logic is to introduce
<strong>probabilities</strong> into the picture of semantic models for deductive logic. The
idea is that if we can talk about the probabilities of the premises and
conclusion in our semantics, we can directly translate the idea of the premises
making the conclusion (more) likely into our model. We will illustrate these
ideas by focusing on a stripped-down formal language that only represents the
logical form of &ldquo;and&rdquo; sentences.</p>
<p>Assume that we have access to absolutely all possible models of our formal
language, just like before with deductive logic. Remember from section <a href="#semantic-methods-for-deduction">2.3.1
Semantic methods for deduction</a> that for a
sentence $A$, the notation $[A]$ denotes the set of all the possible reasoning
scenarios where $A$ is true. Similarly, we have the same definition of logical
conjunction $$[A\land B] = [A]\cap[B]$$ within this background set of all
models.</p>
<p>In this setting, we introduce the probabilities as numeric measures of how
likely it is that a given sentence is actually true.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> For a
sentence $A$, we write $$Pr(A)$$ to denote this measure. You can think of
$Pr(A)$ as a measure of how likely it is that the way things actually are is
among the possibilities $[A]$ according to which $A$ is true.</p>
<p>There are a number of laws that probabilities satisfy. For example, the value of
$Pr(A)$ must always be between $0$ and $1$ (inclusive): $$0\leq Pr(A)\leq
1.$$ We&rsquo;ll study these laws later in the book, when we return to probabilities
and inductive reasoning. For now, a simplified interpretation of the numbers
$Pr(A)$ works just fine.</p>
<p>For now, you can think of $Pr(A)$ as measuring the proportion of all possible
scenarios that are $A$-scenarios. So, if $A$ is the sentence &ldquo;Robots can fly&rdquo;,
$Pr(A)$ measures how many scenarios of all scenarios are ones where robots can
fly. Assuming that all scenarios are equally likely, then if robots can fly in
half of all scenarios, then $Pr(A)=0.5$. And if robots can fly in
$\frac{3}{4}$ of all possible scenarios, then $Pr(A)=0.75$. And so on.</p>
<p>Now that we have gotten some idea of probabilities in this way, we can talk
about <strong>conditional probabilities</strong>: how likely it is that something&rsquo;s true
<em>assuming that</em> something else is true. Remember from <a href="#hypothetically">above</a>
that validity if a hypothetical concept. So to properly capture inductive
validity, we need a notion of <em>hypothetical probability</em>.</p>
<p>Mathematically, we write $Pr(B|A)$ for the conditional probability of $B$ given
$A$. Intuitively,  $Pr(B|A)$ is a measure of how likely it is that $B$ is true
assuming that the actual scenario is an $A$-scenario. That is, if $A$ is the
sentence &ldquo;Pigs can fly&rdquo;, for example, and $B$ is &ldquo;horses can fly&rdquo;, then
$Pr(B|A)$ asks how likely it is that horses can fly, assuming that pigs can fly.</p>
<p>But how should we define this mathematically? It turns out that assuming the
simplified interpretation of probabilities as proportion of scenarios from
before gives us a very natural answer. The idea is to say that :
$$Pr(B|A)=\frac{Pr(A\land B)}{Pr(A)}.$$ Basically, what this definition says is
that $Pr(B|A)$ is a measure of the proportion of $A$-scenarios that are
$B$-scenarios (look at the intersection diagram to see this immediately).</p>
<p>With this out of the way, we get a very natural model of inductive validity by
saying that: $$ P_1,P_2,\dots\mid\approx C \text{ if, and only if, }
Pr(C|P_1\land P_2\land \dots)\geq Pr(C)$$ In words, the inference from $P_1,P_2,
\dots$ to $C$ is inductively valid just in case the conditional probability of
$C$ given $P_1,P_2, \dots$ is higher than the probability of $C$ not assuming
the premises. <a href="https://en.wikipedia.org/wiki/Rudolf_Carnap"
  target="_blank">Rudolf Carnap</a>
coined the term &ldquo;increase of firmness&rdquo; for this relation.</p>
<p>In fact, in this model we can also measure <em>how much</em> the premises raise the
probability of the conclusion. This is simply $Pr(C|P_1\land P_2\land
\dots)-Pr(C)$. Using this idea, we can define a <strong>strong inductive inference</strong>
as one where the conditional probability of the conclusion given the premises is
<em>much</em> larger ($\gg$) than that of the conclusion:
$$P_1, P_2, \dots\stackrel{!}{\mid\approx} C \text{just in case }Pr(C|P_1\land P_2\land
\dots) \gg Pr(C)$$</p>
<p>Let&rsquo;s think about this model for inductive validity. Using it, we can establish
some useful facts about inductive inferences. As before, we will only sketch the
explanations or proofs of these facts. We start off with some commonalities
between inductive and deductive logic.</p>
<p>For example, we can easily show that:</p>
<ul>
<li>$A\land B \mid\approx A$</li>
</ul>
<p><em>Sketch of an Explanation</em>: This is inductively valid because the proportion of
$A\land B$-scenarios which are $A$-scenarios is $1$: <em>all</em> $A\land B$-scenarios
are $A$-scenarios. That is $Pr(A|A\land B)=1$. Since $0\leq Pr(A)\leq 1$, we
know that $Pr(A|A\land B)\geq Pr(A)$.</p>
<p><em>Question</em>: is the inference always inductively <em>strong</em>?</p>
<p>But, crucially, we get that:</p>
<ul>
<li>It is possible to have $P_1,P_2,\dots \mid\approx C$, but not
$Q,P_1,P_2,\dots\mid\approx C$.</li>
</ul>
<p>This says that <strong>monotonicity fails for induction</strong>. In other words, it can be valid
to infer a conclusion from certain premises, but invalid to infer the exact same
conclusion after &ldquo;adding new information&rdquo; to those premises. New information
changes the conclusion we can inductively infer.</p>
<p><em>Sketch of an Explanation</em>: Suppose you live in a town with a few vegans. You
know that all those people ride bicycles, they do not own cars.</p>
<p>In the diagram below, $Pr(A)$ is the probability of being a cyclist, $Pr(B)$ is
the probability of eating meat, and $Pr(C)$ is the probability of being vegan:</p>


<img src="https://logicalmethods.ai/textbook/valid-inference/img/probabilities.jpg" class="img-thumbnail"  alt="" >


<p>For a randomly chosen person, $Pr(C)$ is not very high since the proportion of
vegans, $C$, among the general population is low.</p>
<p>At the same time, $Pr(C|A)$ is relatively high, <em>of the cyclists</em>, $A$, a much
larger proportion is vegan than in the general population. This means that
$Pr(C|A) &gt; P(C)$, and so $A\mid\approx C$. Or, in words, a randomly chosen
person being a cyclist is inductive evidence for that person being vegan.</p>
<p>But if we look at the proportion of bikers that are cyclist and meat eaters,
obviously, <em>none</em> of them is a vegan&ndash;they&rsquo;re meat eaters, after all. This means
that $Pr(C| A\land B)=0$. But since $Pr(C)$ is positive&mdash;there are <em>some</em>
vegans, this means that $Pr(C| A\land B)\ngeq Pr(C)$ and so $A,B\mid\approx C$
fails.</p>
<p>The <strong>non-monotonicity</strong> of induction is very important. Perhaps a certain
element of your beliefs implies a conclusion at this moment, but if you acquire
new information then your <em>updated</em> beliefs might not imply the same conclusion
anymore. This is why we say that inductive rules are <em>defeasible</em>. At one
moment, in one context, it might be inductively good to infer a conclusion based
on some limited information. At the next moment, based on new information, it is
no longer inductively good to infer that same conclusion. You have to &rsquo;take
back&rsquo; the previous &lsquo;outcome&rsquo; of your inferences.</p>
<h2 id="further-readings">Further readings<button class="btn btn-back-to-top">
    
    <i class="bi bi-chevron-double-up"></i>
    
  </button>
</h2>
<hr>

<p>A modern classic on the mathematical definition of deductive validity:</p>
<ul>
<li><a href="https://doi.org/10.2307/2267371"
  target="_blank">A. Tarski. &ldquo;Über den Begriff der logischen Folgerung&rdquo;. <em>Actes du Congrès
International de Philosophie Scientifique</em>. Hermann, Paris, 1936.</a></li>
</ul>
<p>Update of the Tarskian method, presented at an introductory level:</p>
<ul>
<li><a href="https://www.cambridge.org/core/books/logical-consequence/CCF10B5A87373CB40897424453D863A8"
  target="_blank">G. Sher. <em>Logical Consequence</em>. Elements in Philosophy and Logic. Cambridge
University Press, 2022.</a></li>
</ul>
<p>Early work on AI implementations of inductive reasoning:</p>
<ul>
<li><a href="https://doi.org/10.1016/0004-3702%2880%2990011-9"
  target="_blank">J. McCarthy. &ldquo;Circumscription: A form of non-monotonic reasoning”, <em>Artificial
Intelligence</em>, 13: 27–39.</a></li>
</ul>
<p>The idea of using probability theory to model inductive validity is heavily
influenced by the work of Rudolf Carnap:</p>
<ul>
<li><a href="https://doi.org/10.1086/286851"
  target="_blank">R. Carnap. &ldquo;On inductive logic&rdquo;, <em>Philosophy of Science</em> 12(2):
72-97.</a></li>
</ul>
<p>Influential mathematical treatment of inductive validity:</p>
<ul>
<li><a href="https://doi.org/10.1016/0004-3702%2890%2990101-5"
  target="_blank">S. Kraus, D. Lehmann, &amp; M. Magidor. &ldquo;Nonmonotonic Reasoning, Preferential
Models and Cumulative Logics&rdquo;. <em>Artifical Intelligence</em>, 44: 167–207,
1990.</a>.</li>
</ul>
<p><strong>Notes:</strong></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>If you stopped here and thought &ldquo;yes, but <em>how do we</em> define all
possible models for a language?&rdquo; Don&rsquo;t worry. We will explain this in detail
later.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Why only a &ldquo;first step&rdquo;? Well, the main reason is that we know it
is possible for an inference to use many premises. However, the way we are
setting up this definition of validity, it only applies to inferences that have
a single premise sentence. It is not a full definition.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>In probability theory, there are many different ways of
thinking about what probabilities &ldquo;really are&rdquo;: subjective estimates, objective
chances, frequencies, or something else altogether. Here we avoid this question
and treat probabilities intuitively. We return to the &ldquo;nature&rdquo; of probabilities
later in the book.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
 <p><em>Last edited: </em>09/09/2024</p>
</div>


<ul class="navbar-nav flex-row justify-content-between mt-auto">
  
  <li class="nav-item">
    <a href="/textbook/logic-and-ai/" class="btn" tabindex="-1" role="button" aria-disabled="true">
      <i class="bi bi-chevron-left"></i>
    </a>
    
  </li>
  <li class="nav-item">
    <button class="btn btn-back-to-top">
    <i class="bi bi-chevron-double-up"></i>
  </button>
  </li>
  <li class="nav-item">
    
    <a href="/textbook/formal-languages/" class="btn" tabindex="-1" role="button" aria-disabled="true">
      <i class="bi bi-chevron-right"></i>
    </a>
    
  </li>
</ul>

    </main>
    <footer class="align-self-center d-flex flex-column mt-md-1 p-0" data-bs-theme="dark">
      <nav class="navbar bg-dark shadow" data-bs-theme="dark" aria-label="footer-navbar">
  <div class="container-sm align-self-center px-3">
    <a class="navbar-brand fs-6" href="/about" aria-label="copyright-link">&copy; 2024 jkorbmacher et al. </a>
    <ul class="navbar-nav">
      <li class="nav-item">
        <a class="nav-link fs-6" href="https://github.com/jkorb/logicalmethods.ai" target="_blank"><i class="bi bi-github"></i></a>
      </li>
    </ul>
  </div>
</nav>
 
      <script src="https://logicalmethods.ai/js/helpers.js"></script>
      <script src="https://logicalmethods.ai/bootstrap/dist/js/bootstrap.bundle.min.js"></script>
      
      
    </footer>
  </body>
</html>
